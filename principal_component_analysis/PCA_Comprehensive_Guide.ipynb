{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåå The Art of Seeing the Invisible: A Comprehensive Guide to Principal Component Analysis\n",
    "\n",
    "**Subtitle**: *Transforming High-Dimensional Chaos into Crystal-Clear Insights*\n",
    "\n",
    "---\n",
    "\n",
    "> *\"When the wise person cannot see the forest for the trees, they don't cut down the forest‚Äîthey learn to see it from above.\"*  \n",
    "> ‚Äî **Ancient wisdom, modern relevance**\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Prologue: The Curse That Haunts All Data Scientists\n",
    "\n",
    "Picture yourself as a **detective** investigating a crime scene. But here's the twist: instead of working in 3 dimensions (length, width, height), you must analyze evidence across **30 dimensions**‚Äî30 different measurements for each suspect.\n",
    "\n",
    "- üëÅÔ∏è **Your eyes**: Evolved to see only in 3D\n",
    "- üß† **Your brain**: Overwhelmed trying to comprehend 30 variables simultaneously\n",
    "- ‚è±Ô∏è **Your computer**: Slowing to a crawl under the computational burden\n",
    "- üìä **Your model**: Confused by noise and swimming in redundancy\n",
    "\n",
    "**This is the Curse of Dimensionality**‚Äîa silent killer in machine learning.\n",
    "\n",
    "| Dimensions | What Happens |\n",
    "|------------|---------------------|\n",
    "| 3‚Äì10 | Manageable and intuitive‚Äîhumans can visualize this |\n",
    "| 10‚Äì50 | Correlations hide deep insights; visualization becomes difficult |\n",
    "| 50‚Äì1,000 | \"Big p, small n\"‚Äîmore features than samples; the model overfits |\n",
    "| 1,000+ | A computational nightmare; overfitting is virtually guaranteed |\n",
    "\n",
    "> **Here's where PCA enters as a hero**: Not by destroying information, but by **reorganizing reality itself**‚Äîfinding the signal buried beneath the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≠ Act I: The Philosophy Behind PCA\n",
    "\n",
    "### What is PCA, Really?\n",
    "\n",
    "Rather than launch into equations, let's understand PCA through everyday parallels:\n",
    "\n",
    "| Analogy | The Deeper Truth |\n",
    "|---------|-----------|\n",
    "| **The Photographer** | Finding the single perfect angle that captures a 3D statue's essence in a 2D photograph |\n",
    "| **The Chef** | Reducing 30 ingredients down to 5 core flavor profiles that preserve the dish's soul |\n",
    "| **The Cartographer** | Projecting our round Earth onto flat paper‚Äîsurrendering some truth, gaining immense usability |\n",
    "| **The Composer** | Identifying the few musical themes that make a symphony instantly recognizable |\n",
    "\n",
    "> **The Core Insight**: *Discover new directions (axes) in your data where patterns reveal themselves most dramatically‚Äîthen discard the rest.*\n",
    "\n",
    "### The Four Pillars of PCA Understanding\n",
    "\n",
    "```\n",
    "üßò THE FOUR PILLARS OF PCA:\n",
    "\n",
    "Pillar 1: CORRELATION IS REDUNDANCY\n",
    "    ‚Ü≥ When two features move together in lockstep, they're telling the same story\n",
    "    ‚Ü≥ This is waste‚Äîinformation repeated twice\n",
    "    \n",
    "Pillar 2: VARIANCE EQUALS INFORMATION  \n",
    "    ‚Ü≥ Features that vary widely across samples distinguish them from each other\n",
    "    ‚Ü≥ Features that barely change are background noise\n",
    "    \n",
    "Pillar 3: ORTHOGONALITY IS PURITY\n",
    "    ‚Ü≥ New axes must point in perpendicular directions (uncorrelated)\n",
    "    ‚Ü≥ No overlap, no redundancy, complete independence\n",
    "    \n",
    "Pillar 4: PROJECTION PRESERVES ESSENCE\n",
    "    ‚Ü≥ Projecting data onto the top-K axes retains the essential patterns\n",
    "    ‚Ü≥ Discarding low-variance axes removes noise, not information\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ Act II: The Mathematics Unveiled (With Intuition First)\n",
    "\n",
    "### Visualizing PCA Geometrically\n",
    "\n",
    "Imagine your data as a **cloud of points** floating in 30-dimensional space. You cannot see it directly. But mathematics gives us sight.\n",
    "\n",
    "**The Journey Unfolds in Four Steps:**\n",
    "\n",
    "**Step 1: Center the Cloud**\n",
    "```python\n",
    "# Shift every point so the cloud's center aligns with the origin\n",
    "X_centered = X - np.mean(X, axis=0)\n",
    "```\n",
    "> *\"Every great journey begins at the center, from a place of balance.\"*\n",
    "\n",
    "**Step 2: Discover the Direction of Greatest Spread**\n",
    "```python\n",
    "# Find the direction where data varies the most\n",
    "# This becomes Principal Component 1 (PC1)\n",
    "# Mathematically: the eigenvector with the largest eigenvalue\n",
    "```\n",
    "\n",
    "**Step 3: Find Perpendicular Directions of the Next Greatest Spread**\n",
    "```python\n",
    "# Each new component must be orthogonal (perpendicular) to all previous ones\n",
    "# Like finding the optimal camera angles, one by one\n",
    "# Result: PC2, PC3, ... PC30‚Äîeach uncorrelated with the others\n",
    "```\n",
    "\n",
    "**Step 4: Keep Only the Top K Directions**\n",
    "```python\n",
    "# Project all data points onto your top K components\n",
    "# Result: A K-dimensional representation that preserves the story\n",
    "# The transformation is linear, reversible (approximately), and elegant\n",
    "```\n",
    "\n",
    "### The Mathematical Heart of PCA\n",
    "\n",
    "$$\\text{Transformed Data} = W_K^T \\cdot X_{\\text{centered}}$$\n",
    "\n",
    "Where:\n",
    "- $W_K$ = a matrix containing our top-K eigenvectors (the principal components)\n",
    "- Each column is a **new axis** in the transformed space\n",
    "- The transformation is **linear and reversible**‚Äîwe don't destroy information, we reorganize it\n",
    "\n",
    "### Explained Variance: The Trust Metric\n",
    "\n",
    "This table shows how much of your data's story is told by each component:\n",
    "\n",
    "| Component | Variance Explained | Running Total |\n",
    "|-----------|-------------------|------------|\n",
    "| PC1 | 45% | 45% |\n",
    "| PC2 | 25% | 70% |\n",
    "| PC3 | 15% | 85% |\n",
    "| PC4‚Äì30 | 15% | 100% |\n",
    "\n",
    "> **The Insight**: If just PC1 and PC2 capture 70% of the variance, you've preserved 70% of your data's meaningful information in just 2 dimensions! That's the power of PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Going Deeper: The Mathematical Foundations\n",
    "\n",
    "For those ready to explore the rigorous core of PCA:\n",
    "\n",
    "### The Covariance Matrix: The Blueprint\n",
    "\n",
    "PCA's mission is to diagonalize the covariance matrix $C$:\n",
    "$$C = \\frac{1}{n-1} X^T X$$\n",
    "(assuming $X$ is centered)\n",
    "\n",
    "Think of the off-diagonal elements as **redundancy detectors**. Diagonalizing this matrix means rotating your coordinate system so that off-diagonal elements become zero‚Äîmeaning zero correlation, zero redundancy, pure independence.\n",
    "\n",
    "### Eigenvectors vs. Singular Value Decomposition\n",
    "\n",
    "While textbooks teach eigendecomposition of the covariance matrix, modern implementations (including scikit-learn) use **Singular Value Decomposition (SVD)** for superior numerical stability.\n",
    "\n",
    "Given a centered data matrix $X$:\n",
    "$$X = U \\Sigma V^T$$\n",
    "\n",
    "The columns of $V$ (right singular vectors) ARE the principal components‚Äîidentical to the eigenvectors of the covariance matrix. The singular values relate to eigenvalues through:\n",
    "$$\\lambda_i = \\frac{\\sigma_i^2}{n-1}$$\n",
    "\n",
    "**Why SVD over eigendecomposition?** SVD is more numerically stable and handles edge cases gracefully. This is why scikit-learn's PCA implementation is both robust and lightning-fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öîÔ∏è Act III: When PCA Becomes Your Secret Weapon\n",
    "\n",
    "### The Perfect Battlegrounds for PCA\n",
    "\n",
    "PCA shines brightest in these scenarios:\n",
    "\n",
    "| Scenario | Why PCA Wins | Real-World Example |\n",
    "|----------|--------------|--------------|\n",
    "| **Visualization** | Reduce 1,000 dimensions to 2D for human perception | Creating visualizations of high-dimensional datasets |\n",
    "| **Noise Reduction** | Low-variance components typically capture noise, not signal | Cleaning grainy images before processing |\n",
    "| **Speed Optimization** | Fewer features means faster training and prediction | Real-time recommendation systems |\n",
    "| **Multicollinearity** | Removes correlated features that confuse models | Financial models with redundant economic indicators |\n",
    "| **Data Compression** | Store or transmit less data without losing essence | Face recognition (Eigenfaces), image compression |\n",
    "| **Pattern Discovery** | Uncover latent factors | Psychology surveys revealing underlying personality traits |\n",
    "\n",
    "### The Complete Strength-Weakness Picture\n",
    "\n",
    "```\n",
    "‚úÖ WHEN PCA SHINES:\n",
    "   ‚îú‚îÄ Removes redundancy (multicollinearity)\n",
    "   ‚îú‚îÄ Reduces overfitting (fewer dimensions = simpler models)\n",
    "   ‚îú‚îÄ Accelerates training and inference speed\n",
    "   ‚îú‚îÄ Enables visualization of complex data\n",
    "   ‚îú‚îÄ Filters noise from signal (low-variance = noise)\n",
    "   ‚îî‚îÄ Works unsupervised (no labels needed)\n",
    "\n",
    "‚ùå WHEN PCA STRUGGLES:\n",
    "   ‚îú‚îÄ Misses non-linear patterns (PCA is fundamentally linear)\n",
    "   ‚îú‚îÄ Sacrifices interpretability (components are abstract combinations)\n",
    "   ‚îú‚îÄ Demands scaling (results depend heavily on standardization)\n",
    "   ‚îú‚îÄ Assumes high variance = important (sometimes false assumption)\n",
    "   ‚îú‚îÄ Component directions lack intuitive meaning (signs can flip)\n",
    "   ‚îî‚îÄ Performs poorly on very small datasets\n",
    "```\n",
    "\n",
    "### When to Avoid PCA\n",
    "\n",
    "**Don't use PCA for:**\n",
    "- **Image classification**: Convolutional Neural Networks need spatial structure that PCA destroys\n",
    "- **Small datasets**: When n < p, regularization is safer than dimensionality reduction\n",
    "- **Feature importance**: When you need to understand which original features matter most\n",
    "- **Categorical data**: Without proper encoding (one-hot), PCA will fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé™ Act IV: PCA in Action (Interactive Demonstrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== √∞≈∏≈Ω¬™ THE PCA TOOLKIT ==========\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.datasets import load_breast_cancer, make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Beautiful styling\n",
    "sns.set_theme(style='whitegrid', palette='husl', font_scale=1.1)\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "print(\"√∞≈∏≈í≈í The PCA Analysis Laboratory is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== √∞≈∏≈Ω¬® ACT IV, SCENE 1: The Perfect Illusion ==========\n",
    "# Visual demonstration with synthetic data\n",
    "\n",
    "np.random.seed(42)\n",
    "# Create highly correlated 2D data (elongated blob)\n",
    "mean = [0, 0]\n",
    "cov = [[3, 2.5], [2.5, 2]]  # Strong correlation\n",
    "X_demo = np.random.multivariate_normal(mean, cov, 300)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Original data\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_demo[:, 0], X_demo[:, 1], alpha=0.6, s=50, c='steelblue')\n",
    "plt.title('√∞≈∏‚Äú≈† Original Data: Correlated & Redundant\\n(2 features, but 1 story)', fontsize=14)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.axis('equal')\n",
    "\n",
    "# Apply PCA\n",
    "pca_demo = PCA(n_components=2)\n",
    "X_pca_demo = pca_demo.fit_transform(X_demo)\n",
    "\n",
    "# Plot PCA transformed data\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_pca_demo[:, 0], X_pca_demo[:, 1], alpha=0.6, s=50, c='coral')\n",
    "plt.title('√¢≈ì¬® After PCA: Uncorrelated & Aligned\\n(PC1 = max variance, PC2 = perpendicular)', fontsize=14)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"√∞≈∏‚Äù¬ç Original correlation: {np.corrcoef(X_demo[:,0], X_demo[:,1])[0,1]:.3f}\")\n",
    "print(f\"√¢≈ì¬® After PCA correlation: {np.corrcoef(X_pca_demo[:,0], X_pca_demo[:,1])[0,1]:.6f} (√¢‚Ä∞ÀÜ0!)\")\n",
    "print(f\"√∞≈∏‚ÄúÀÜ Variance captured by PC1: {pca_demo.explained_variance_ratio_[0]:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== √∞≈∏≈Ω¬Ø ACT IV, SCENE 2: The Eigenvector Revelation ==========\n",
    "# Show the principal components as vectors\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_demo[:, 0], X_demo[:, 1], alpha=0.4, s=50, c='lightblue', label='Data points')\n",
    "\n",
    "# Plot principal components as arrows\n",
    "scale = 3\n",
    "for i, (comp, var) in enumerate(zip(pca_demo.components_, pca_demo.explained_variance_)):\n",
    "    comp_scale = comp * np.sqrt(var) * scale\n",
    "    plt.arrow(0, 0, comp_scale[0], comp_scale[1], \n",
    "             head_width=0.2, head_length=0.2, \n",
    "             fc='red' if i == 0 else 'green', \n",
    "             ec='red' if i == 0 else 'green',\n",
    "             linewidth=3, label=f'PC{i+1} ({pca_demo.explained_variance_ratio_[i]:.1%})')\n",
    "\n",
    "plt.title('√∞≈∏≈Ω¬Ø Principal Components: The New Axes of Reality\\n(Red = PC1, Green = PC2)', fontsize=16)\n",
    "plt.xlabel('Original Feature 1')\n",
    "plt.ylabel('Original Feature 2')\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"√∞≈∏¬ß¬≠ PC1 points in the direction of maximum spread\")\n",
    "print(\"√∞≈∏¬ß¬≠ PC2 is perpendicular √¢‚Ç¨‚Äù captures remaining variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèõÔ∏è Act V: The Real-World Arena ‚Äì Breast Cancer Classification Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== √∞≈∏¬è‚Ä∫√Ø¬∏¬è LOAD THE 30-DIMENSIONAL ARENA ==========\n",
    "cancer = load_breast_cancer(as_frame=True)\n",
    "df = cancer.frame\n",
    "\n",
    "print(\"√∞≈∏¬è‚Ä∫√Ø¬∏¬è Welcome to the Breast Cancer Dimensionality Colosseum!\")\n",
    "print(f\"√¢≈°‚Äù√Ø¬∏¬è Warriors (samples): {len(df)}\")\n",
    "print(f\"√∞≈∏‚Äú≈† Battlefields (features): {len(cancer.feature_names)}\")\n",
    "print(f\"√∞≈∏≈Ω¬Ø Mission: Classify malignant vs benign tumors\")\n",
    "print(f\"\\nFeature names: {cancer.feature_names[:5]}... (30 total)\")\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "print(f\"\\nClass distribution:\\n{df['target'].value_counts(normalize=True).round(3)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== √∞≈∏‚Äù¬• THE CURSE VISUALIZED ==========\n",
    "plt.figure(figsize=(14, 12))\n",
    "corr_matrix = X.corr()\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, cmap='RdBu_r', center=0, \n",
    "           square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8}, annot=False)\n",
    "plt.title('√∞≈∏‚Äù¬• The Curse of Dimensionality: Feature Correlation Web\\n(30 features, many redundant)', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "print(\"√∞≈∏‚Äù¬¥ Red = highly correlated (redundant information)\")\n",
    "print(\"√∞≈∏‚Äù¬µ Blue = negatively correlated\")\n",
    "print(\"√¢≈°¬™ White = uncorrelated\")\n",
    "print(\"\\n√∞≈∏‚Äô¬° PCA will collapse these correlations into pure, independent components!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== √¢≈°‚Äù√Ø¬∏¬è PHASE 1: BASELINE MODEL (NO PCA) ==========\n",
    "print(\"=\" * 60)\n",
    "print(\"√¢≈°‚Äù√Ø¬∏¬è PHASE 1: The Baseline Battle (30 features)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Baseline model\n",
    "start_time = time.time()\n",
    "model_base = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_base.fit(X_train_scaled, y_train)\n",
    "train_time_base = time.time() - start_time\n",
    "\n",
    "y_pred_base = model_base.predict(X_test_scaled)\n",
    "acc_base = accuracy_score(y_test, y_pred_base)\n",
    "\n",
    "print(f\"\\n√∞≈∏‚Ä∫¬°√Ø¬∏¬è Baseline Results (Full 30D):\")\n",
    "print(f\"   Accuracy: {acc_base:.4f} ({acc_base*100:.2f}%)\")\n",
    "print(f\"   Training time: {train_time_base:.4f} seconds\")\n",
    "print(f\"   Model size: {X_train_scaled.shape[1]} features\")\n",
    "\n",
    "print(f\"\\n√∞≈∏‚Äú‚Äπ Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_base, target_names=['Malignant', 'Benign']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== √∞≈∏‚Äù¬Æ PHASE 2: THE PCA RITUAL ==========\n",
    "print(\"=\" * 60)\n",
    "print(\"√∞≈∏‚Äù¬Æ PHASE 2: The Dimensionality Reduction Ritual\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Fit PCA on training data only!\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_train_scaled)\n",
    "\n",
    "# Explained variance analysis\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(1, 31), pca_full.explained_variance_ratio_, alpha=0.7, color='skyblue', edgecolor='navy')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('√∞≈∏‚Äú≈† Variance Explained by Each Component\\n(Scree Plot)')\n",
    "plt.xticks(range(1, 31, 2))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, 31), cumulative_variance, 'bo-', linewidth=3, markersize=8, color='darkred')\n",
    "plt.axhline(y=0.90, color='green', linestyle='--', label='90% threshold')\n",
    "plt.axhline(y=0.95, color='orange', linestyle='--', label='95% threshold')\n",
    "plt.axhline(y=0.99, color='purple', linestyle='--', label='99% threshold')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('√∞≈∏‚ÄúÀÜ Cumulative Variance Explained\\n(How many components to keep?)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal components\n",
    "n_90 = np.argmax(cumulative_variance >= 0.90) + 1\n",
    "n_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "n_99 = np.argmax(cumulative_variance >= 0.99) + 1\n",
    "\n",
    "print(f\"\\n√∞≈∏≈Ω¬Ø The Analysis Speaks:\")\n",
    "print(f\"   90% variance: {n_90} components ({cumulative_variance[n_90-1]:.2%})\")\n",
    "print(f\"   95% variance: {n_95} components ({cumulative_variance[n_95-1]:.2%}) √¢¬≠¬ê SWEET SPOT\")\n",
    "print(f\"   99% variance: {n_99} components ({cumulative_variance[n_99-1]:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== √¢≈°‚Äù√Ø¬∏¬è PHASE 3: THE GREAT COMPARISON ==========\n",
    "print(\"=\" * 60)\n",
    "print(\"√¢≈°‚Äù√Ø¬∏¬è PHASE 3: The Great Battle √¢‚Ç¨‚Äù Different Dimensionalities\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = []\n",
    "components_to_test = [2, 5, 10, n_90, n_95, 30]  # 30 = baseline\n",
    "\n",
    "for n_comp in components_to_test:\n",
    "    print(f\"\\n√∞≈∏‚Äù¬¨ Testing with {n_comp} components...\")\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca_n = PCA(n_components=n_comp)\n",
    "    X_train_pca = pca_n.fit_transform(X_train_scaled)\n",
    "    X_test_pca = pca_n.transform(X_test_scaled)\n",
    "    \n",
    "    # Train model\n",
    "    start = time.time()\n",
    "    model_pca = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model_pca.fit(X_train_pca, y_train)\n",
    "    train_time = time.time() - start\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model_pca.predict(X_test_pca)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'Components': n_comp,\n",
    "        'Accuracy': acc,\n",
    "        'Accuracy Drop': acc_base - acc,\n",
    "        'Time (s)': train_time,\n",
    "        'Speedup': train_time_base / train_time,\n",
    "        'Variance Kept': np.sum(pca_n.explained_variance_ratio_)\n",
    "    })\n",
    "    \n",
    "    print(f\"   Accuracy: {acc:.4f} | Time: {train_time:.4f}s | Variance: {np.sum(pca_n.explained_variance_ratio_):.2%}\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"√∞≈∏‚Äú≈† THE VERDICT: Speed vs Accuracy Trade-off\")\n",
    "print(\"=\" * 80)\n",
    "print(results_df.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== √∞≈∏‚ÄúÀÜ PHASE 4: VISUALIZATION MASTERY ==========\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Accuracy vs Components\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(results_df['Components'], results_df['Accuracy'], 'go-', linewidth=3, markersize=10)\n",
    "ax1.axhline(y=acc_base, color='red', linestyle='--', label=f'Baseline ({acc_base:.3f})')\n",
    "ax1.set_xlabel('Number of Components')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('√∞≈∏≈Ω¬Ø Accuracy vs Dimensionality\\n(How much can we compress?)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Training Time vs Components\n",
    "ax2 = axes[0, 1]\n",
    "ax2.bar(results_df['Components'].astype(str), results_df['Time (s)'], color='coral', edgecolor='darkred')\n",
    "ax2.set_xlabel('Number of Components')\n",
    "ax2.set_ylabel('Training Time (seconds)')\n",
    "ax2.set_title('√¢¬è¬±√Ø¬∏¬è Speed vs Dimensionality\\n(Faster is better)')\n",
    "\n",
    "# 3. Speedup ratio\n",
    "ax3 = axes[1, 0]\n",
    "colors = ['green' if x > 1 else 'red' for x in results_df['Speedup']]\n",
    "ax3.bar(results_df['Components'].astype(str), results_df['Speedup'], color=colors, edgecolor='black')\n",
    "ax3.axhline(y=1, color='black', linestyle='--', label='Baseline speed')\n",
    "ax3.set_xlabel('Number of Components')\n",
    "ax3.set_ylabel('Speedup Factor')\n",
    "ax3.set_title('√∞≈∏≈°‚Ç¨ Speedup vs Baseline\\n(Green = faster, Red = slower)')\n",
    "\n",
    "# 4. 2D PCA Visualization (The Money Shot)\n",
    "ax4 = axes[1, 1]\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_train_2d = pca_2d.fit_transform(X_train_scaled)\n",
    "scatter = ax4.scatter(X_train_2d[:, 0], X_train_2d[:, 1], c=y_train, cmap='coolwarm', \n",
    "                     alpha=0.7, s=100, edgecolors='black', linewidth=0.5)\n",
    "ax4.set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.1%})')\n",
    "ax4.set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.1%})')\n",
    "ax4.set_title(f'√∞≈∏‚Äî¬∫√Ø¬∏¬è Data in 2D PCA Space\\n(Only 2 features, yet separable!)')\n",
    "plt.colorbar(scatter, ax=ax4, label='Malignant (0) vs Benign (1)')\n",
    "\n",
    "plt.suptitle('√∞≈∏‚Äú≈† PCA Trade-off Analysis Dashboard', fontsize=18, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== √∞≈∏≈Ω‚Äú PHASE 5: THE KEY INSIGHTS ==========\n",
    "print(\"=\" * 80)\n",
    "print(\"√∞≈∏≈Ω‚Äú KEY INSIGHTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_idx = results_df['Accuracy Drop'].abs().idxmin()\n",
    "best_row = results_df.loc[best_idx]\n",
    "\n",
    "print(f\"\\n√∞≈∏¬è‚Ä† OPTIMAL CONFIGURATION:\")\n",
    "print(f\"   Components: {int(best_row['Components'])}\")\n",
    "print(f\"   Accuracy: {best_row['Accuracy']:.4f} (drop of {best_row['Accuracy Drop']:.4f})\")\n",
    "print(f\"   Speedup: {best_row['Speedup']:.2f}x faster\")\n",
    "print(f\"   Variance preserved: {best_row['Variance Kept']:.2%}\")\n",
    "\n",
    "print(f\"\\n√∞≈∏‚Äô¬° KEY LEARNINGS:\")\n",
    "print(f\"   √¢‚Ç¨¬¢ Reduced {X.shape[1]}D √¢‚Ä†‚Äô {int(best_row['Components'])}D ({X.shape[1]/best_row['Components']:.1f}x compression)\")\n",
    "print(f\"   √¢‚Ç¨¬¢ Lost only {best_row['Accuracy Drop']*100:.2f}% accuracy\")\n",
    "print(f\"   √¢‚Ç¨¬¢ Gained {best_row['Speedup']:.2f}x training speed\")\n",
    "print(f\"   √¢‚Ç¨¬¢ Removed multicollinearity (components are orthogonal)\")\n",
    "\n",
    "print(f\"\\n√∞≈∏≈Ω¬Ø BUSINESS VALUE:\")\n",
    "print(f\"   √¢‚Ç¨¬¢ Deploy lighter models (edge devices, mobile)\")\n",
    "print(f\"   √¢‚Ç¨¬¢ Faster predictions in production\")\n",
    "print(f\"   √¢‚Ç¨¬¢ Reduced storage requirements\")\n",
    "print(f\"   √¢‚Ç¨¬¢ Visualizable for human oversight\")\n",
    "\n",
    "# Feature importance in PCA space\n",
    "print(f\"\\n√∞≈∏‚Äù¬ç TOP 3 ORIGINAL FEATURES CONTRIBUTING TO PC1:\")\n",
    "components_df = pd.DataFrame(\n",
    "    pca_full.components_[:3].T,\n",
    "    columns=['PC1', 'PC2', 'PC3'],\n",
    "    index=cancer.feature_names\n",
    ")\n",
    "print(components_df['PC1'].abs().sort_values(ascending=False).head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Act VI: Your Decision Framework\n",
    "\n",
    "### PCA vs. Alternative Dimensionality Reduction Techniques\n",
    "\n",
    "Choosing the right technique is crucial. Here's how they compare:\n",
    "\n",
    "| Technique | Best For | Avoid If | Learning Curve |\n",
    "|-----------|----------|----------|------------|\n",
    "| **PCA** | Linear relationships, speed, interpretability | Non-linear patterns exist, or minimal data | ‚≠ê Easy |\n",
    "| **t-SNE** | Stunning 2D visualizations, non-linear clusters | Working with large datasets or needing reproducibility | ‚≠ê‚≠ê‚≠ê Steep |\n",
    "| **UMAP** | Visualization preserving both local and global structure | Perfect reproducibility is critical, or you need predictions on new data | ‚≠ê‚≠ê Moderate |\n",
    "| **Autoencoders** | Non-linear compression, learning deep latent features | Need interpretability, have minimal training data | ‚≠ê‚≠ê‚≠ê‚≠ê Very Steep |\n",
    "| **Feature Selection** | Retaining original feature interpretability | Dealing with highly correlated features | ‚≠ê Easy |\n",
    "\n",
    "### The Pre-PCA Checklist\n",
    "\n",
    "Before applying PCA, ensure you've covered these bases:\n",
    "\n",
    "```\n",
    "‚úÖ MUST DO:\n",
    "   ‚òê Standardize all features (zero mean, unit variance) ‚Äî ABSOLUTELY CRITICAL!\n",
    "   ‚òê Handle missing values appropriately\n",
    "   ‚òê Analyze explained variance‚Äîdecide on K before transforming\n",
    "   ‚òê Benchmark your model with and without PCA\n",
    "   ‚òê Fit PCA on training data ONLY; transform test data separately\n",
    "\n",
    "‚ùå NEVER DO:\n",
    "   ‚òê Apply PCA to raw, unscaled data\n",
    "   ‚òê Fit PCA on the full dataset (includes test data‚Äîcauses data leakage!)\n",
    "   ‚òê Use PCA when interpretability of original features is paramount\n",
    "   ‚òê Expect PCA to improve accuracy (it often doesn't‚Äîit trades speed for accuracy)\n",
    "   ‚òê Apply to purely categorical data without proper encoding\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì The 10 Commandments of Dimensionality Reduction\n",
    "\n",
    "These principles will guide you through every PCA challenge:\n",
    "\n",
    "```\n",
    "1Ô∏è‚É£  STANDARDIZE EVERYTHING\n",
    "    Before PCA, ensure zero mean and unit variance for all features\n",
    "    \n",
    "2Ô∏è‚É£  NEVER LEAK DATA\n",
    "    Fit PCA on training data only; transform test data separately\n",
    "    \n",
    "3Ô∏è‚É£  UNDERSTAND YOUR DATA\n",
    "    Always analyze explained variance‚Äîlet the data speak first\n",
    "    \n",
    "4Ô∏è‚É£  BENCHMARK RUTHLESSLY\n",
    "    Compare model performance before and after PCA\n",
    "    \n",
    "5Ô∏è‚É£  REMEMBER PCA'S PURPOSE\n",
    "    Use it for speed and visualization, not just accuracy improvement\n",
    "    \n",
    "6Ô∏è‚É£  EMBRACE LINEARITY'S LIMITS\n",
    "    PCA handles linear relationships; use Kernel PCA for curved patterns\n",
    "    \n",
    "7Ô∏è‚É£  VISUALIZE YOUR RESULTS\n",
    "    When possible, create 2D plots to understand what PCA discovered\n",
    "    \n",
    "8Ô∏è‚É£  CONSIDER ALTERNATIVES\n",
    "    t-SNE and UMAP excel at visualization; PCA excels at production\n",
    "    \n",
    "9Ô∏è‚É£  DOCUMENT YOUR CHOICES\n",
    "    Record how much variance you kept‚Äîexplain your K selection\n",
    "    \n",
    "üîü  STAY PRAGMATIC\n",
    "    Don't sacrifice 5%+ accuracy for less than 2x speed improvement\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== √∞≈∏‚Äô¬æ SAVE & DEPLOY ==========\n",
    "# Save the optimal PCA pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from joblib import dump\n",
    "\n",
    "optimal_pca = PCA(n_components=n_95)\n",
    "optimal_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', optimal_pca),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "optimal_pipeline.fit(X_train, y_train)\n",
    "dump(optimal_pipeline, 'pca_breast_cancer_model.joblib')\n",
    "\n",
    "print(\"\"\"\n",
    "√∞≈∏≈Ω‚Äú CHALLENGE:\n",
    "   1. Apply PCA to your own high-dimensional dataset\n",
    "   2. Find the \"elbow\" in explained variance\n",
    "   3. Compare 3 algorithms: Logistic Regression, Random Forest, SVM\n",
    "   4. Build a PCA + Neural Network (autoencoder comparison)\n",
    "   5. Deploy the compressed model as a FastAPI endpoint\n",
    "\n",
    "√∞≈∏‚Äú≈° NEXT STEPS:\n",
    "   √¢‚Ä†‚Äô Anomaly Detection (PCA for outlier detection)\n",
    "   √¢‚Ä†‚Äô Advanced: Kernel PCA (non-linear dimensionality reduction)\n",
    "   √¢‚Ä†‚Äô Advanced: Sparse PCA (feature selection + reduction)\n",
    "   √¢‚Ä†‚Äô Advanced: Incremental PCA (streaming data)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Epilogue: Your Next Chapter\n",
    "\n",
    "### Master PCA Through Practice\n",
    "\n",
    "Ready to deepen your skills? Here are your next challenges:\n",
    "\n",
    "1. **Apply PCA to Your Own Dataset**\n",
    "   - Find the \"elbow\" point in explained variance\n",
    "   - Compare accuracy across different component counts\n",
    "   \n",
    "2. **Algorithm Showdown**\n",
    "   - Train three different models: Logistic Regression, Random Forest, SVM\n",
    "   - Measure speed improvements and accuracy trade-offs with each\n",
    "   \n",
    "3. **Deep Learning Integration**\n",
    "   - Build a PCA + Neural Network pipeline\n",
    "   - Compare with an Autoencoder (non-linear alternative)\n",
    "   \n",
    "4. **Production Deployment**\n",
    "   - Export your PCA model and classifier\n",
    "   - Deploy as a FastAPI endpoint with predictions on new data\n",
    "   \n",
    "5. **Advanced Techniques**\n",
    "   - Explore Kernel PCA for non-linear dimensionality reduction\n",
    "   - Investigate Sparse PCA for feature selection combined with reduction\n",
    "   - Experiment with Incremental PCA for streaming data\n",
    "\n",
    "### Recommended Reading & Resources\n",
    "\n",
    "| Resource | Why It Matters |\n",
    "|----------|----------|\n",
    "| **Python Data Science Handbook** | Best visual intuitive explanations |\n",
    "| **3Blue1Brown: Eigenvectors** | Intuitive mathematical foundation |\n",
    "| **Scikit-Learn Documentation** | The authoritative reference for implementation |\n",
    "| **Pearson (1901) Original Paper** | The historical origin of PCA |\n",
    "| **Sch√∂lkopf et al., 1998** | Kernel PCA‚Äîextending PCA non-linearly |\n",
    "\n",
    "---\n",
    "\n",
    "## üèÜ Final Wisdom\n",
    "\n",
    "PCA is more than an algorithm‚Äîit's a mindset. When facing high-dimensional chaos, remember:\n",
    "\n",
    "> *The goal is not to compress data thoughtlessly, but to find the hidden dimensions where your story truly lives. Sometimes, less is infinitely more.*\n",
    "\n",
    "**Now go forth and reduce dimensions with confidence.** Your high-dimensional datasets await their transformation. üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
