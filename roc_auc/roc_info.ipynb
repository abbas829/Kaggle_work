{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ **The Complete Guide to ROC-AUC for Beginners**\n",
    "## *From Zero to Hero in Classification Metrics*\n",
    "\n",
    "---\n",
    "\n",
    "**Welcome!** This notebook is your ultimate guide to understanding **ROC-AUC** - one of the most important metrics in machine learning classification.\n",
    "\n",
    "### ğŸ“š **What You'll Learn:**\n",
    "| Module | Topic | Duration |\n",
    "|--------|-------|----------|\n",
    "| 1ï¸âƒ£ | What is ROC-AUC? | 10 min |\n",
    "| 2ï¸âƒ£ | The Confusion Matrix Foundation | 15 min |\n",
    "| 3ï¸âƒ£ | TPR vs FPR Deep Dive | 15 min |\n",
    "| 4ï¸âƒ£ | The ROC Curve Construction | 20 min |\n",
    "| 5ï¸âƒ£ | AUC Interpretation & Assumptions | 15 min |\n",
    "| 6ï¸âƒ£ | When to Use vs Other Metrics | 10 min |\n",
    "| 7ï¸âƒ£ | Statistical Tests & EDA | 20 min |\n",
    "| 8ï¸âƒ£ | Practical Implementation | 25 min |\n",
    "\n",
    "**Prerequisites:** Basic Python, Basic Statistics  \n",
    "**Difficulty:** â­ Beginner Friendly  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification, load_breast_cancer, load_digits\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“– **Chapter 1: What is ROC-AUC?**\n",
    "\n",
    "### ğŸ­ **The Story Behind the Name**\n",
    "\n",
    "| Acronym | Full Name | Emoji |\n",
    "|---------|-----------|-------|\n",
    "| **ROC** | **R**eceiver **O**perating **C**haracteristic | ğŸ“¡ |\n",
    "| **AUC** | **A**rea **U**nder the **C**urve | ğŸ“Š |\n",
    "\n",
    "> **History Note:** ROC curves were first developed during **World War II** for radar signal detection! They measured how well radar operators could distinguish enemy aircraft from noise. Today, we use them to distinguish between classes in ML.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ **Simple Definition**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  ROC-AUC measures how well your model distinguishes     â”‚\n",
    "â”‚  between POSITIVE class (1) and NEGATIVE class (0)      â”‚\n",
    "â”‚  across ALL possible classification thresholds.          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ğŸ§  **Analogy: The Medical Test**\n",
    "\n",
    "Imagine a blood test for a disease:\n",
    "- **Perfect Test (AUC = 1.0):** All sick people test positive, all healthy test negative\n",
    "- **Random Test (AUC = 0.5):** Coin flip - no better than guessing\n",
    "- **Worse than Random (AUC < 0.5):** The test is inverted (always wrong!)\n",
    "\n",
    "| AUC Score | Interpretation | Grade |\n",
    "|-----------|----------------|-------|\n",
    "| 0.90 - 1.00 | Excellent ğŸ† | A+ |\n",
    "| 0.80 - 0.90 | Good ğŸ‘ | A |\n",
    "| 0.70 - 0.80 | Fair ğŸ™‚ | B |\n",
    "| 0.60 - 0.70 | Poor ğŸ˜ | C |\n",
    "| 0.50 - 0.60 | Fail âŒ | D |\n",
    "| < 0.50 | Worse than random ğŸ™ƒ | F |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“– **Chapter 2: The Foundation - Confusion Matrix**\n",
    "\n",
    "Before understanding ROC-AUC, we MUST understand the **Confusion Matrix** - it's the building block!\n",
    "\n",
    "### ğŸ”² **The 2Ã—2 Confusion Matrix**\n",
    "\n",
    "```\n",
    "                    PREDICTED\n",
    "                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                 â”‚    1    â”‚    0    â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    " A  â”‚     1      â”‚   TP    â”‚   FN    â”‚\n",
    " C  â”‚  (Actual   â”‚         â”‚         â”‚\n",
    " T  â”‚   Positive)â”‚  âœ“âœ“âœ“    â”‚  âœ—âœ—âœ—    â”‚\n",
    " U  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    " A  â”‚     0      â”‚   FP    â”‚   TN    â”‚\n",
    " L  â”‚  (Actual   â”‚         â”‚         â”‚\n",
    "    â”‚  Negative) â”‚  âœ—âœ—âœ—    â”‚  âœ“âœ“âœ“    â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ğŸ­ **The Four Characters:**\n",
    "\n",
    "| Term | Full Name | Meaning | Emoji |\n",
    "|------|-----------|---------|-------|\n",
    "| **TP** | True Positive | Predicted 1, Actually 1 (Correct!) | âœ… |\n",
    "| **TN** | True Negative | Predicted 0, Actually 0 (Correct!) | âœ… |\n",
    "| **FP** | False Positive | Predicted 1, Actually 0 (Type I Error) | âŒ |\n",
    "| **FN** | False Negative | Predicted 0, Actually 1 (Type II Error) | âŒ |\n",
    "\n",
    "### ğŸ“Š **Create Your First Confusion Matrix**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simple binary classification data\n",
    "np.random.seed(42)\n",
    "y_true = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])  # True labels\n",
    "y_pred = np.array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1])  # Predicted labels\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"ğŸ¯ CONFUSION MATRIX BREAKDOWN:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"True Negatives (TN):  {tn}  âœ… Actually 0, Predicted 0\")\n",
    "print(f\"False Positives (FP): {fp}  âŒ Actually 0, Predicted 1\")\n",
    "print(f\"False Negatives (FN): {fn}  âŒ Actually 1, Predicted 0\")\n",
    "print(f\"True Positives (TP):  {tp}  âœ… Actually 1, Predicted 1\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "            yticklabels=['Actual 0', 'Actual 1'],\n",
    "            annot_kws={\"size\": 20, \"weight\": \"bold\"})\n",
    "plt.title('ğŸ”² Confusion Matrix Visualization', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.ylabel('Actual Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate basic metrics\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "print(f\"\\nğŸ“Š Basic Metrics from Confusion Matrix:\")\n",
    "print(f\"   Accuracy:  {accuracy:.2f}  = (TP+TN)/(Total)\")\n",
    "print(f\"   Precision: {precision:.2f}  = TP/(TP+FP)\")\n",
    "print(f\"   Recall:    {recall:.2f}  = TP/(TP+FN)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“– **Chapter 3: TPR vs FPR - The Heart of ROC**\n",
    "\n",
    "### ğŸ­ **Introducing the Two Main Characters**\n",
    "\n",
    "ROC curve plots two rates against each other:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Y-axis: TPR (True Positive Rate)                       â”‚\n",
    "â”‚         \"How many actual positives did we catch?\"       â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  X-axis: FPR (False Positive Rate)                      â”‚\n",
    "â”‚         \"How many actual negatives did we misclassify?\" â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ğŸ“ **The Formulas**\n",
    "\n",
    "| Metric | Formula | Also Called | Intuition |\n",
    "|--------|---------|-------------|-----------|\n",
    "| **TPR** | $\\frac{TP}{TP + FN}$ | Sensitivity, Recall | Coverage of positive class |\n",
    "| **FPR** | $\\frac{FP}{FP + TN}$ | 1 - Specificity | False alarm rate |\n",
    "| **TNR** | $\\frac{TN}{FP + TN}$ | Specificity | Coverage of negative class |\n",
    "\n",
    "### ğŸ§  **Memory Trick**\n",
    "\n",
    "```\n",
    "TPR = TP / (All Positives)  â†’  \"Of all sick people, how many did we find?\"\n",
    "FPR = FP / (All Negatives)  â†’  \"Of all healthy people, how many did we scare?\"\n",
    "```\n",
    "\n",
    "**Goal:** Maximize TPR (find all sick people) while Minimizing FPR (don't scare healthy people)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate TPR and FPR calculation\n",
    "def calculate_rates(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    tnr = tn / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn,\n",
    "        'TPR (Sensitivity)': tpr,\n",
    "        'FPR (1-Specificity)': fpr,\n",
    "        'TNR (Specificity)': tnr\n",
    "    }\n",
    "\n",
    "# Example calculations\n",
    "print(\"ğŸ” TPR vs FPR Calculation Examples:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Scenario 1: Strict threshold (predicts 1 only when very confident)\n",
    "y_pred_strict = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1])\n",
    "rates_strict = calculate_rates(y_true, y_pred_strict)\n",
    "print(\"\\nğŸŸ¢ Scenario 1: Strict Threshold (Predict '1' rarely)\")\n",
    "for key, val in rates_strict.items():\n",
    "    print(f\"   {key}: {val:.3f}\" if isinstance(val, float) else f\"   {key}: {val}\")\n",
    "\n",
    "# Scenario 2: Lenient threshold (predicts 1 easily)\n",
    "y_pred_lenient = np.array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "rates_lenient = calculate_rates(y_true, y_pred_lenient)\n",
    "print(\"\\nğŸ”´ Scenario 2: Lenient Threshold (Predict '1' easily)\")\n",
    "for key, val in rates_lenient.items():\n",
    "    print(f\"   {key}: {val:.3f}\" if isinstance(val, float) else f\"   {key}: {val}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ’¡ Key Insight: As we make more positive predictions...\")\n",
    "print(f\"   Strict â†’ TPR: {rates_strict['TPR (Sensitivity)']:.2f}, FPR: {rates_strict['FPR (1-Specificity)']:.2f}\")\n",
    "print(f\"   Lenient â†’ TPR: {rates_lenient['TPR (Sensitivity)']:.2f}, FPR: {rates_lenient['FPR (1-Specificity)']:.2f}\")\n",
    "print(\"   We catch more positives (higher TPR) but at cost of more false alarms (higher FPR)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“– **Chapter 4: Building the ROC Curve Step-by-Step**\n",
    "\n",
    "### ğŸ—ï¸ **Construction Process**\n",
    "\n",
    "The ROC curve is built by varying the **classification threshold** from 0 to 1:\n",
    "\n",
    "```\n",
    "Step 1: Get probability scores from model (not just 0/1 predictions)\n",
    "        â†“\n",
    "Step 2: Sort all samples by probability (highest to lowest)\n",
    "        â†“\n",
    "Step 3: For each possible threshold, calculate TPR and FPR\n",
    "        â†“\n",
    "Step 4: Plot FPR (x-axis) vs TPR (y-axis)\n",
    "        â†“\n",
    "Step 5: Calculate AUC = Area under this curve\n",
    "```\n",
    "\n",
    "### ğŸ¨ **Threshold Examples**\n",
    "\n",
    "| Threshold | Rule | Result |\n",
    "|-----------|------|--------|\n",
    "| 1.0 | Predict 1 only if P(1) â‰¥ 1.0 | No predictions of 1 (conservative) |\n",
    "| 0.5 | Predict 1 if P(1) â‰¥ 0.5 | Balanced approach |\n",
    "| 0.0 | Predict 1 if P(1) â‰¥ 0.0 | Always predict 1 (aggressive) |\n",
    "\n",
    "### ğŸ“Š **Visual Demonstration**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a detailed example showing how ROC curve is built\n",
    "np.random.seed(42)\n",
    "n_samples = 20\n",
    "\n",
    "# Generate synthetic data with clear separation\n",
    "X, y = make_classification(n_samples=n_samples, n_features=2, n_redundant=0, \n",
    "                          n_informative=2, n_clusters_per_class=1, \n",
    "                          class_sep=1.5, random_state=42)\n",
    "\n",
    "# Train a simple model\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get probability scores\n",
    "y_proba = model.predict_proba(X)[:, 1]\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "df_viz = pd.DataFrame({\n",
    "    'True_Label': y,\n",
    "    'Predicted_Prob': y_proba\n",
    "}).sort_values('Predicted_Prob', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"ğŸ“‹ Step 1: Probability Scores (Sorted by Confidence)\")\n",
    "print(\"=\"*60)\n",
    "print(df_viz.head(10).to_string())\n",
    "print(f\"\\n... showing {n_samples} total samples\")\n",
    "\n",
    "# Calculate ROC curve points\n",
    "fpr, tpr, thresholds = roc_curve(y, y_proba)\n",
    "\n",
    "# Plot the construction process\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Distribution of scores by class\n",
    "ax1 = axes[0, 0]\n",
    "pos_scores = y_proba[y == 1]\n",
    "neg_scores = y_proba[y == 0]\n",
    "ax1.hist(pos_scores, bins=10, alpha=0.6, label='Positive Class', color='green')\n",
    "ax1.hist(neg_scores, bins=10, alpha=0.6, label='Negative Class', color='red')\n",
    "ax1.set_xlabel('Predicted Probability')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('1ï¸âƒ£ Distribution of Prediction Scores', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: ROC Curve with thresholds marked\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(fpr, tpr, 'b-', linewidth=2, label='ROC Curve')\n",
    "ax2.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Classifier')\n",
    "ax2.scatter(fpr[::max(1, len(fpr)//10)], tpr[::max(1, len(tpr)//10)], \n",
    "           c=thresholds[::max(1, len(thresholds)//10)], cmap='viridis', s=50, zorder=5)\n",
    "ax2.set_xlabel('False Positive Rate (FPR)')\n",
    "ax2.set_ylabel('True Positive Rate (TPR)')\n",
    "ax2.set_title('2ï¸âƒ£ ROC Curve (Colors = Thresholds)', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: How TPR and FPR change with threshold\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(thresholds, tpr, 'g-', label='TPR (Sensitivity)', linewidth=2)\n",
    "ax3.plot(thresholds, fpr, 'r-', label='FPR (1-Specificity)', linewidth=2)\n",
    "ax3.set_xlabel('Classification Threshold')\n",
    "ax3.set_ylabel('Rate')\n",
    "ax3.set_title('3ï¸âƒ£ TPR and FPR vs Threshold', fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: AUC illustration\n",
    "ax4 = axes[1, 1]\n",
    "ax4.fill_between(fpr, tpr, alpha=0.3, color='blue', label=f'AUC = {auc(fpr, tpr):.3f}')\n",
    "ax4.plot(fpr, tpr, 'b-', linewidth=2)\n",
    "ax4.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "ax4.set_xlabel('False Positive Rate (FPR)')\n",
    "ax4.set_ylabel('True Positive Rate (TPR)')\n",
    "ax4.set_title('4ï¸âƒ£ Area Under Curve (AUC)', fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_construction.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… AUC Score: {roc_auc_score(y, y_proba):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“– **Chapter 5: AUC - Area Under Curve Deep Dive**\n",
    "\n",
    "### ğŸ“ **What Does AUC Actually Mean?**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  AUC = P(Model ranks random positive higher than       â”‚\n",
    "â”‚        random negative)                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**In Plain English:** If you randomly pick one positive and one negative sample, AUC is the probability that your model will give the positive sample a higher score than the negative sample.\n",
    "\n",
    "### ğŸ¯ **Probabilistic Interpretation**\n",
    "\n",
    "| AUC | Interpretation |\n",
    "|-----|----------------|\n",
    "| 0.5 | Model can't distinguish between classes (random guessing) |\n",
    "| 0.7 | 70% chance model ranks a random positive above a random negative |\n",
    "| 0.9 | 90% chance of correct ranking |\n",
    "| 1.0 | Perfect ranking (no overlap between classes) |\n",
    "\n",
    "### ğŸ“ **Geometric Interpreture**\n",
    "\n",
    "```\n",
    "Perfect Classifier (AUC = 1.0):\n",
    "    TPR\n",
    "    1.0 â”¤    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
    "        â”‚    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
    "    0.5 â”¤    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
    "        â”‚    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
    "    0.0 â”¼â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "        0.0  0.5        1.0 FPR\n",
    "    \n",
    "Random Classifier (AUC = 0.5):\n",
    "    TPR\n",
    "    1.0 â”¤              â–ˆâ–ˆ\n",
    "        â”‚           â–ˆâ–ˆ   \n",
    "    0.5 â”¤        â–ˆâ–ˆ      \n",
    "        â”‚     â–ˆâ–ˆ         \n",
    "    0.0 â”¼â”€â”€â–ˆâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "        0.0  0.5        1.0 FPR\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate AUC interpretation\n",
    "def demonstrate_auc_interpretation(n_pairs=1000):\n",
    "    \"\"\"\n",
    "    Demonstrate that AUC = probability of correct ranking\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate scores\n",
    "    y_true = np.array([0] * 500 + [1] * 500)\n",
    "    # Class 1 has higher scores on average\n",
    "    scores = np.concatenate([\n",
    "        np.random.normal(0.3, 0.2, 500),  # Class 0\n",
    "        np.random.normal(0.7, 0.2, 500)   # Class 1\n",
    "    ])\n",
    "    \n",
    "    # Calculate AUC\n",
    "    auc_score = roc_auc_score(y_true, scores)\n",
    "    \n",
    "    # Empirical test: Random pairs\n",
    "    correct_rankings = 0\n",
    "    pos_indices = np.where(y_true == 1)[0]\n",
    "    neg_indices = np.where(y_true == 0)[0]\n",
    "    \n",
    "    for _ in range(n_pairs):\n",
    "        pos_idx = np.random.choice(pos_indices)\n",
    "        neg_idx = np.random.choice(neg_indices)\n",
    "        \n",
    "        if scores[pos_idx] > scores[neg_idx]:\n",
    "            correct_rankings += 1\n",
    "    \n",
    "    empirical_prob = correct_rankings / n_pairs\n",
    "    \n",
    "    return auc_score, empirical_prob, scores, y_true\n",
    "\n",
    "auc_score, empirical_prob, scores, y_true = demonstrate_auc_interpretation()\n",
    "\n",
    "print(\"ğŸ”¬ EMPIRICAL VERIFICATION OF AUC INTERPRETATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Calculated AUC:        {auc_score:.4f}\")\n",
    "print(f\"Empirical Probability: {empirical_prob:.4f}\")\n",
    "print(f\"Difference:            {abs(auc_score - empirical_prob):.4f}\")\n",
    "print(\"\\nThis confirms: AUC = P(Score_Positive > Score_Negative)\")\n",
    "\n",
    "# Visualize score distributions\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "pos_scores = scores[y_true == 1]\n",
    "neg_scores = scores[y_true == 0]\n",
    "plt.hist(neg_scores, bins=30, alpha=0.6, label=f'Negative (n={len(neg_scores)})', color='red')\n",
    "plt.hist(pos_scores, bins=30, alpha=0.6, label=f'Positive (n={len(pos_scores)})', color='green')\n",
    "plt.axvline(x=np.median(neg_scores), color='red', linestyle='--', label='Median Neg')\n",
    "plt.axvline(x=np.median(pos_scores), color='green', linestyle='--', label='Median Pos')\n",
    "plt.xlabel('Model Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Score Distributions by Class', fontweight='bold')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "fpr, tpr, _ = roc_curve(y_true, scores)\n",
    "plt.fill_between(fpr, tpr, alpha=0.3, color='blue')\n",
    "plt.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC Curve (AUC = {auc_score:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random (AUC = 0.5)')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve', fontweight='bold')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“– **Chapter 6: Assumptions of ROC-AUC**\n",
    "\n",
    "### âš ï¸ **Critical Assumptions**\n",
    "\n",
    "| Assumption | Explanation | Violation Consequences |\n",
    "|------------|-------------|------------------------|\n",
    "| **1. Binary Classification** | Only works with two classes | Use multi-class extensions (OvR) |\n",
    "| **2. Probabilistic Scores** | Needs continuous scores, not just 0/1 | Use calibration techniques |\n",
    "| **3. Independent Samples** | Samples should be i.i.d. | Time series require adjustments |\n",
    "| **4. Sufficient Sample Size** | Needs enough samples for stable estimate | High variance in small samples |\n",
    "| **5. Class Overlap** | Assumes some separability is possible | AUC=0.5 means no signal |\n",
    "\n",
    "### ğŸ­ **When AUC Can Be Misleading**\n",
    "\n",
    "```\n",
    "âŒ Class Imbalance Extremes:\n",
    "   - If 99% are negative, high AUC might not translate to good precision\n",
    "   - A model predicting all negatives has undefined precision but good specificity\n",
    "   \n",
    "âŒ Different Misclassification Costs:\n",
    "   - AUC treats FP and FN equally\n",
    "   - Medical diagnosis: Missing cancer (FN) is worse than false alarm (FP)\n",
    "   - Use cost-sensitive metrics instead\n",
    "   \n",
    "âŒ Threshold Selection:\n",
    "   - AUC averages over all thresholds\n",
    "   - You might only care about one specific operating point\n",
    "```\n",
    "\n",
    "### ğŸ“Š **Comparison with Other Metrics**\n",
    "\n",
    "| Metric | Best For | Limitation | Use When |\n",
    "|--------|----------|------------|----------|\n",
    "| **ROC-AUC** | Overall ranking ability | Ignores calibration | Balanced classes, ranking tasks |\n",
    "| **PR-AUC** | Imbalanced data | Sensitive to class imbalance | Rare positive class (fraud, disease) |\n",
    "| **F1-Score** | Balance precision/recall | Single threshold only | You need a specific decision threshold |\n",
    "| **Accuracy** | Balanced, equal costs | Misleading if imbalanced | Classes are balanced and costs equal |\n",
    "| **Log-Loss** | Calibrated probabilities | Penalizes confident wrong predictions | You need probability estimates |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ROC-AUC vs PR-AUC in imbalanced scenarios\n",
    "def compare_metrics_imbalance(pos_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Compare ROC-AUC and PR-AUC under class imbalance\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    n_pos = int(n_samples * pos_ratio)\n",
    "    n_neg = n_samples - n_pos\n",
    "    \n",
    "    # Generate imbalanced data\n",
    "    y_true = np.array([1]*n_pos + [0]*n_neg)\n",
    "    # Weak classifier\n",
    "    scores = np.concatenate([\n",
    "        np.random.beta(2, 5, n_pos),  # Positives: lower scores on average\n",
    "        np.random.beta(5, 2, n_neg)   # Negatives: higher scores on average\n",
    "    ])\n",
    "    \n",
    "    # Invert scores to make it a decent classifier\n",
    "    scores = 1 - scores\n",
    "    \n",
    "    roc_auc = roc_auc_score(y_true, scores)\n",
    "    pr_auc = average_precision_score(y_true, scores)\n",
    "    \n",
    "    return roc_auc, pr_auc, y_true, scores\n",
    "\n",
    "# Test different imbalance ratios\n",
    "ratios = [0.5, 0.2, 0.1, 0.05, 0.01]\n",
    "results = []\n",
    "\n",
    "for ratio in ratios:\n",
    "    roc_auc, pr_auc, y_true, scores = compare_metrics_imbalance(ratio)\n",
    "    results.append({\n",
    "        'Pos_Ratio': ratio,\n",
    "        'ROC_AUC': roc_auc,\n",
    "        'PR_AUC': pr_auc,\n",
    "        'Difference': roc_auc - pr_auc\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"ğŸ“Š ROC-AUC vs PR-AUC Under Class Imbalance\")\n",
    "print(\"=\"*60)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(results_df['Pos_Ratio'], results_df['ROC_AUC'], 'o-', label='ROC-AUC', linewidth=2, markersize=8)\n",
    "plt.plot(results_df['Pos_Ratio'], results_df['PR_AUC'], 's-', label='PR-AUC', linewidth=2, markersize=8)\n",
    "plt.xlabel('Positive Class Ratio')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Metric Behavior vs Class Imbalance', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xscale('log')\n",
    "\n",
    "# Example with 10% positive class\n",
    "roc_auc, pr_auc, y_true, scores = compare_metrics_imbalance(0.1)\n",
    "fpr, tpr, _ = roc_curve(y_true, scores)\n",
    "precision, recall, _ = precision_recall_curve(y_true, scores)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC={roc_auc:.3f})', linewidth=2)\n",
    "plt.plot(recall, precision, label=f'PR Curve (AP={pr_auc:.3f})', linewidth=2)\n",
    "plt.xlabel('FPR / Recall')\n",
    "plt.ylabel('TPR / Precision')\n",
    "plt.title('ROC vs PR Curve (10% Positive)', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Key Insight: As imbalance increases, PR-AUC drops more dramatically than ROC-AUC\")\n",
    "print(\"   Use PR-AUC when dealing with highly imbalanced data!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“– **Chapter 7: Best Data Types for ROC-AUC**\n",
    "\n",
    "### âœ… **Ideal Scenarios for ROC-AUC**\n",
    "\n",
    "| Data Characteristic | Why ROC-AUC Works Well | Example |\n",
    "|--------------------|------------------------|---------|\n",
    "| **Balanced Classes** | Both classes contribute equally | Customer churn (50/50) |\n",
    "| **Ranking Important** | Relative ordering matters | Search results ranking |\n",
    "| **Threshold Agnostic** | No specific operating point | Medical screening tests |\n",
    "| **Probabilistic Output** | Model outputs probabilities | Logistic regression, Neural networks |\n",
    "| **Equal Misclassification Cost** | FP and FN equally bad | Generic binary classification |\n",
    "\n",
    "### âŒ **When to Avoid ROC-AUC**\n",
    "\n",
    "| Scenario | Better Alternative | Reason |\n",
    "|----------|-------------------|--------|\n",
    "| **Severe Imbalance (1:100+)** | PR-AUC, F1-score | ROC-AUC can be optimistic |\n",
    "| **Need Specific Threshold** | F1, Precision, Recall | AUC averages all thresholds |\n",
    "| **Different Costs (FP vs FN)** | Cost-sensitive metrics | AUC weighs errors equally |\n",
    "| **Multi-class (no single positive)** | Multi-class log-loss, Macro-F1 | AUC designed for binary |\n",
    "| **Calibration Critical** | Brier Score, Log-loss | AUC measures ranking, not calibration |\n",
    "\n",
    "### ğŸ“Š **Data Type Compatibility Matrix**\n",
    "\n",
    "| Data Type | ROC-AUC Suitability | Notes |\n",
    "|-----------|---------------------|-------|\n",
    "| Tabular/Numerical | â­â­â­â­â­ | Excellent, use tree-based models |\n",
    "| Text/Embeddings | â­â­â­â­â˜† | Good with neural networks |\n",
    "| Images | â­â­â­â­â˜† | Good, but often use accuracy |\n",
    "| Time Series | â­â­â­â˜†â˜† | Needs temporal validation split |\n",
    "| Graphs | â­â­â­â˜†â˜† | Use specialized graph metrics |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate ROC-AUC on different data types\n",
    "print(\"ğŸ¯ ROC-AUC PERFORMANCE ACROSS DATA TYPES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Clean Tabular Data\n",
    "X_clean, y_clean = make_classification(n_samples=1000, n_features=20, \n",
    "                                       n_informative=15, n_redundant=5,\n",
    "                                       random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_clean, y_clean, test_size=0.3, random_state=42)\n",
    "\n",
    "model_clean = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_clean.fit(X_train, y_train)\n",
    "proba_clean = model_clean.predict_proba(X_test)[:, 1]\n",
    "auc_clean = roc_auc_score(y_test, proba_clean)\n",
    "print(f\"âœ… Clean Tabular Data:     AUC = {auc_clean:.4f}\")\n",
    "\n",
    "# 2. Noisy Data (less separable)\n",
    "X_noisy, y_noisy = make_classification(n_samples=1000, n_features=20, \n",
    "                                       n_informative=5, n_redundant=15,\n",
    "                                       flip_y=0.3, random_state=42)\n",
    "X_train_n, X_test_n, y_train_n, y_test_n = train_test_split(X_noisy, y_noisy, test_size=0.3, random_state=42)\n",
    "\n",
    "model_noisy = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_noisy.fit(X_train_n, y_train_n)\n",
    "proba_noisy = model_noisy.predict_proba(X_test_n)[:, 1]\n",
    "auc_noisy = roc_auc_score(y_test_n, proba_noisy)\n",
    "print(f\"âš ï¸  Noisy Data:            AUC = {auc_noisy:.4f}\")\n",
    "\n",
    "# 3. Real Dataset (Breast Cancer)\n",
    "data = load_breast_cancer()\n",
    "X_real, y_real = data.data, data.target\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_real, y_real, test_size=0.3, random_state=42)\n",
    "\n",
    "model_real = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_real.fit(X_train_r, y_train_r)\n",
    "proba_real = model_real.predict_proba(X_test_r)[:, 1]\n",
    "auc_real = roc_auc_score(y_test_r, proba_real)\n",
    "print(f\"ğŸ¥ Medical Data (Cancer):  AUC = {auc_real:.4f}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "scenarios = ['Clean\\nTabular', 'Noisy\\nData', 'Medical\\n(Cancer)']\n",
    "aucs = [auc_clean, auc_noisy, auc_real]\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "bars = plt.bar(scenarios, aucs, color=['green', 'orange', 'blue'], alpha=0.7)\n",
    "plt.ylim(0.5, 1.0)\n",
    "plt.ylabel('ROC-AUC Score')\n",
    "plt.title('AUC Across Data Types', fontweight='bold')\n",
    "for bar, auc in zip(bars, aucs):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{auc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# ROC Curves comparison\n",
    "plt.subplot(1, 3, 2)\n",
    "fpr_c, tpr_c, _ = roc_curve(y_test, proba_clean)\n",
    "fpr_n, tpr_n, _ = roc_curve(y_test_n, proba_noisy)\n",
    "fpr_r, tpr_r, _ = roc_curve(y_test_r, proba_real)\n",
    "\n",
    "plt.plot(fpr_c, tpr_c, label=f'Clean (AUC={auc_clean:.3f})', linewidth=2)\n",
    "plt.plot(fpr_n, tpr_n, label=f'Noisy (AUC={auc_noisy:.3f})', linewidth=2)\n",
    "plt.plot(fpr_r, tpr_r, label=f'Medical (AUC={auc_real:.3f})', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves Comparison', fontweight='bold')\n",
    "plt.legend()\n",
    "\n",
    "# Feature importance for clean data\n",
    "plt.subplot(1, 3, 3)\n",
    "importances = model_clean.feature_importances_\n",
    "indices = np.argsort(importances)[-10:]  # Top 10\n",
    "plt.barh(range(10), importances[indices], color='green', alpha=0.7)\n",
    "plt.yticks(range(10), [f'Feature {i}' for i in indices])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 10 Feature Importances\\n(Clean Data)', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“– **Chapter 8: Statistical Tests for ROC-AUC**\n",
    "\n",
    "### ğŸ”¬ **Essential Statistical Tests**\n",
    "\n",
    "When comparing two models using ROC-AUC, we need statistical significance tests:\n",
    "\n",
    "| Test | Purpose | When to Use |\n",
    "|------|---------|-------------|\n",
    "| **DeLong's Test** | Compare two correlated ROC curves | Same test set, different models |\n",
    "| **Hanley-McNeil Test** | Compare two independent AUCs | Different test sets |\n",
    "| **Bootstrap Confidence Intervals** | Estimate AUC uncertainty | Small sample sizes |\n",
    "| **Cross-Validation** | Estimate generalization | Model selection |\n",
    "\n",
    "### ğŸ“Š **Bootstrap Confidence Interval (Most Practical)**\n",
    "\n",
    "```\n",
    "Step 1: Take B bootstrap samples (e.g., 1000) from your data\n",
    "Step 2: Calculate AUC for each sample\n",
    "Step 3: Use percentiles (2.5th, 97.5th) for 95% CI\n",
    "```\n",
    "\n",
    "### âš–ï¸ **Comparing Two Models**\n",
    "\n",
    "To claim Model A is better than Model B:\n",
    "1. Check if confidence intervals overlap\n",
    "2. Perform paired t-test on CV scores\n",
    "3. Verify on held-out test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_auc(y_true, y_scores, n_bootstrap=1000, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Calculate bootstrap confidence interval for AUC\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(42)\n",
    "    n_samples = len(y_true)\n",
    "    auc_scores = []\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        # Bootstrap sample\n",
    "        indices = rng.randint(0, n_samples, n_samples)\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            continue  # Skip if only one class present\n",
    "        \n",
    "        auc = roc_auc_score(y_true[indices], y_scores[indices])\n",
    "        auc_scores.append(auc)\n",
    "    \n",
    "    auc_scores = np.array(auc_scores)\n",
    "    mean_auc = np.mean(auc_scores)\n",
    "    \n",
    "    # Confidence interval\n",
    "    alpha = 1 - confidence_level\n",
    "    lower = np.percentile(auc_scores, alpha/2 * 100)\n",
    "    upper = np.percentile(auc_scores, (1 - alpha/2) * 100)\n",
    "    \n",
    "    return mean_auc, lower, upper, auc_scores\n",
    "\n",
    "# Generate data\n",
    "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train two models\n",
    "model1 = LogisticRegression(random_state=42)\n",
    "model2 = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "model1.fit(X_train, y_train)\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "proba1 = model1.predict_proba(X_test)[:, 1]\n",
    "proba2 = model2.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate bootstrap CIs\n",
    "print(\"ğŸ”¬ BOOTSTRAP CONFIDENCE INTERVALS (95%)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "auc1 = roc_auc_score(y_test, proba1)\n",
    "mean1, lower1, upper1, scores1 = bootstrap_auc(y_test, proba1)\n",
    "print(f\"Logistic Regression:\")\n",
    "print(f\"   Point Estimate: {auc1:.4f}\")\n",
    "print(f\"   Bootstrap Mean: {mean1:.4f}\")\n",
    "print(f\"   95% CI: [{lower1:.4f}, {upper1:.4f}]\")\n",
    "print(f\"   Margin of Error: Â±{(upper1-lower1)/2:.4f}\")\n",
    "\n",
    "auc2 = roc_auc_score(y_test, proba2)\n",
    "mean2, lower2, upper2, scores2 = bootstrap_auc(y_test, proba2)\n",
    "print(f\"\\nRandom Forest:\")\n",
    "print(f\"   Point Estimate: {auc2:.4f}\")\n",
    "print(f\"   Bootstrap Mean: {mean2:.4f}\")\n",
    "print(f\"   95% CI: [{lower2:.4f}, {upper2:.4f}]\")\n",
    "print(f\"   Margin of Error: Â±{(upper2-lower2)/2:.4f}\")\n",
    "\n",
    "# Statistical comparison\n",
    "diff = auc2 - auc1\n",
    "print(f\"\\nğŸ“Š Comparison (RF - LR): {diff:+.4f}\")\n",
    "if upper1 < lower2 or upper2 < lower1:\n",
    "    print(\"âœ… Statistically significant difference (CIs don't overlap)\")\n",
    "else:\n",
    "    print(\"âš ï¸  No statistically significant difference (CIs overlap)\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(scores1, bins=30, alpha=0.6, label='Logistic Regression', color='blue', edgecolor='black')\n",
    "plt.hist(scores2, bins=30, alpha=0.6, label='Random Forest', color='green', edgecolor='black')\n",
    "plt.axvline(auc1, color='blue', linestyle='--', linewidth=2, label=f'LR AUC = {auc1:.3f}')\n",
    "plt.axvline(auc2, color='green', linestyle='--', linewidth=2, label=f'RF AUC = {auc2:.3f}')\n",
    "plt.xlabel('AUC Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Bootstrap Distribution of AUC Scores', fontweight='bold')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "models = ['Logistic\\nRegression', 'Random\\nForest']\n",
    "means = [mean1, mean2]\n",
    "errors = [[mean1-lower1, mean2-lower2], [upper1-mean1, upper2-mean2]]\n",
    "colors = ['blue', 'green']\n",
    "\n",
    "x_pos = np.arange(len(models))\n",
    "for i, (model, mean, color) in enumerate(zip(models, means, colors)):\n",
    "    plt.errorbar(x_pos[i], mean, yerr=[[errors[0][i]], [errors[1][i]]], \n",
    "                 fmt='o', color=color, capsize=10, capthick=2, markersize=10)\n",
    "    plt.text(x_pos[i], mean, f'{mean:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.xticks(x_pos, models)\n",
    "plt.ylabel('AUC Score')\n",
    "plt.title('95% Confidence Intervals', fontweight='bold')\n",
    "plt.ylim(min(lower1, lower2)-0.05, max(upper1, upper2)+0.05)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“– **Chapter 9: EDA Checklist for ROC-AUC**\n",
    "\n",
    "### âœ… **Pre-Modeling EDA Checklist**\n",
    "\n",
    "Before training your model, perform these checks:\n",
    "\n",
    "| Check | Why It Matters | Tool/Method |\n",
    "|-------|----------------|-------------|\n",
    "| **1. Class Balance** | Imbalance affects AUC interpretation | `value_counts()`, pie chart |\n",
    "| **2. Feature Distributions** | Overlap between classes affects AUC | KDE plots, histograms |\n",
    "| **3. Missing Values** | Can bias model and AUC | `isnull().sum()`, heatmap |\n",
    "| **4. Outliers** | Extreme values skew probabilities | Box plots, Z-score |\n",
    "| **5. Correlations** | Redundant features don't help | Correlation matrix |\n",
    "| **6. Linear Separability** | If linearly separable, any model gets AUC=1 | Scatter plots |\n",
    "\n",
    "### ğŸ¯ **Post-Modeling EDA Checklist**\n",
    "\n",
    "After training, validate your AUC:\n",
    "\n",
    "| Check | Purpose | How |\n",
    "|-------|---------|-----|\n",
    "| **1. ROC Curve Shape** | Check for irregularities (overfitting) | Visual inspection |\n",
    "| **2. Calibration** | Are probabilities accurate? | Reliability diagram |\n",
    "| **3. Threshold Analysis** | Best operating point for your use case | Youden's J statistic |\n",
    "| **4. Cross-Validation** | Is AUC stable across folds? | `cross_val_score` |\n",
    "| **5. Class-wise Performance** | AUC might hide poor performance on one class | Per-class metrics |\n",
    "\n",
    "### ğŸ› ï¸ **Automated EDA Function**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_auc_eda(X, y, feature_names=None):\n",
    "    \"\"\"\n",
    "    Comprehensive EDA checklist for ROC-AUC preparation\n",
    "    \"\"\"\n",
    "    if feature_names is None:\n",
    "        feature_names = [f'Feature_{i}' for i in range(X.shape[1])]\n",
    "    \n",
    "    df = pd.DataFrame(X, columns=feature_names)\n",
    "    df['target'] = y\n",
    "    \n",
    "    print(\"ğŸ” COMPREHENSIVE EDA FOR ROC-AUC\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. Class Balance\n",
    "    print(\"\\nğŸ“Š 1. CLASS BALANCE\")\n",
    "    balance = pd.Series(y).value_counts(normalize=True)\n",
    "    print(f\"   Class 0: {balance[0]:.2%}\")\n",
    "    print(f\"   Class 1: {balance[1]:.2%}\")\n",
    "    imbalance_ratio = max(balance) / min(balance)\n",
    "    print(f\"   Imbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
    "    if imbalance_ratio > 10:\n",
    "        print(\"   âš ï¸  High imbalance detected! Consider PR-AUC instead.\")\n",
    "    elif imbalance_ratio > 3:\n",
    "        print(\"   âš ï¸  Moderate imbalance. Monitor both ROC-AUC and PR-AUC.\")\n",
    "    else:\n",
    "        print(\"   âœ… Balanced classes. ROC-AUC is appropriate.\")\n",
    "    \n",
    "    # 2. Feature Statistics by Class\n",
    "    print(\"\\nğŸ“ˆ 2. FEATURE SEPARABILITY\")\n",
    "    separability_scores = []\n",
    "    for col in feature_names:\n",
    "        pos_mean = df[df['target']==1][col].mean()\n",
    "        neg_mean = df[df['target']==0][col].mean()\n",
    "        pos_std = df[df['target']==1][col].std()\n",
    "        neg_std = df[df['target']==0][col].std()\n",
    "        \n",
    "        # Cohen's d (effect size)\n",
    "        pooled_std = np.sqrt((pos_std**2 + neg_std**2) / 2)\n",
    "        cohens_d = abs(pos_mean - neg_mean) / pooled_std if pooled_std > 0 else 0\n",
    "        separability_scores.append((col, cohens_d))\n",
    "    \n",
    "    separability_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    print(\"   Top 5 most separable features (Cohen's d):\")\n",
    "    for feat, score in separability_scores[:5]:\n",
    "        print(f\"   - {feat}: {score:.3f}\")\n",
    "    \n",
    "    # 3. Missing Values\n",
    "    print(\"\\nâ“ 3. MISSING VALUES\")\n",
    "    missing = df.isnull().sum().sum()\n",
    "    if missing > 0:\n",
    "        print(f\"   âš ï¸  {missing} missing values detected\")\n",
    "    else:\n",
    "        print(\"   âœ… No missing values\")\n",
    "    \n",
    "    # 4. Correlation with Target\n",
    "    print(\"\\nğŸ”— 4. FEATURE-TARGET CORRELATIONS\")\n",
    "    correlations = df.corr()['target'].drop('target').abs().sort_values(ascending=False)\n",
    "    print(\"   Top 5 correlations with target:\")\n",
    "    for feat, corr in correlations.head(5).items():\n",
    "        print(f\"   - {feat}: {corr:.3f}\")\n",
    "    \n",
    "    # 5. Visualizations\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    # Class balance\n",
    "    ax1 = axes[0, 0]\n",
    "    balance.plot(kind='bar', ax=ax1, color=['skyblue', 'orange'])\n",
    "    ax1.set_title('1. Class Balance', fontweight='bold')\n",
    "    ax1.set_ylabel('Proportion')\n",
    "    \n",
    "    # Feature distribution (most separable)\n",
    "    ax2 = axes[0, 1]\n",
    "    top_feat = separability_scores[0][0]\n",
    "    df[df['target']==0][top_feat].hist(ax=ax2, alpha=0.6, bins=20, label='Class 0', color='skyblue')\n",
    "    df[df['target']==1][top_feat].hist(ax=ax2, alpha=0.6, bins=20, label='Class 1', color='orange')\n",
    "    ax2.set_title(f'2. Distribution of {top_feat}', fontweight='bold')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Correlation heatmap (sample)\n",
    "    ax3 = axes[0, 2]\n",
    "    sample_feats = [item[0] for item in separability_scores[:5]] + ['target']\n",
    "    corr_matrix = df[sample_feats].corr()\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, ax=ax3)\n",
    "    ax3.set_title('3. Feature Correlations', fontweight='bold')\n",
    "    \n",
    "    # Box plots for top 2 features\n",
    "    ax4 = axes[1, 0]\n",
    "    df_melted = df[[separability_scores[0][0], separability_scores[1][0], 'target']].melt(\n",
    "        id_vars='target', var_name='Feature', value_name='Value')\n",
    "    sns.boxplot(data=df_melted, x='Feature', y='Value', hue='target', ax=ax4)\n",
    "    ax4.set_title('4. Feature Distributions by Class', fontweight='bold')\n",
    "    \n",
    "    # Scatter plot of top 2 features\n",
    "    ax5 = axes[1, 1]\n",
    "    scatter = ax5.scatter(df[separability_scores[0][0]], df[separability_scores[1][0]], \n",
    "                         c=y, cmap='coolwarm', alpha=0.6, edgecolors='k', linewidth=0.5)\n",
    "    ax5.set_xlabel(separability_scores[0][0])\n",
    "    ax5.set_ylabel(separability_scores[1][0])\n",
    "    ax5.set_title('5. Class Separation in 2D', fontweight='bold')\n",
    "    plt.colorbar(scatter, ax=ax5)\n",
    "    \n",
    "    # Cumulative distribution\n",
    "    ax6 = axes[1, 2]\n",
    "    for cls, color in zip([0, 1], ['skyblue', 'orange']):\n",
    "        subset = df[df['target'] == cls][separability_scores[0][0]]\n",
    "        sorted_data = np.sort(subset)\n",
    "        cumulative = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "        ax6.plot(sorted_data, cumulative, label=f'Class {cls}', color=color, linewidth=2)\n",
    "    ax6.set_xlabel(separability_scores[0][0])\n",
    "    ax6.set_ylabel('Cumulative Probability')\n",
    "    ax6.set_title('6. CDF by Class', fontweight='bold')\n",
    "    ax6.legend()\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'imbalance_ratio': imbalance_ratio,\n",
    "        'separability_scores': separability_scores,\n",
    "        'top_features': [item[0] for item in separability_scores[:5]]\n",
    "    }\n",
    "\n",
    "# Run EDA on sample data\n",
    "X_eda, y_eda = make_classification(n_samples=1000, n_features=10, n_informative=5, \n",
    "                                   n_redundant=2, flip_y=0.1, random_state=42)\n",
    "eda_results = comprehensive_auc_eda(X_eda, y_eda)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“– **Chapter 10: Practical Implementation Guide**\n",
    "\n",
    "### ğŸ› ï¸ **Complete Workflow**\n",
    "\n",
    "Here's the step-by-step process for using ROC-AUC in your project:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  PHASE 1: DATA PREPARATION                                  â”‚\n",
    "â”‚  â”œâ”€â”€ 1. Load and clean data                                 â”‚\n",
    "â”‚  â”œâ”€â”€ 2. Check class balance (use EDA function above)       â”‚\n",
    "â”‚  â”œâ”€â”€ 3. Handle missing values                               â”‚\n",
    "â”‚  â””â”€â”€ 4. Split: Train (70%) / Validation (15%) / Test (15%)  â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  PHASE 2: MODEL TRAINING                                    â”‚\n",
    "â”‚  â”œâ”€â”€ 1. Choose algorithm (Logistic, RF, XGBoost, etc.)      â”‚\n",
    "â”‚  â”œâ”€â”€ 2. Use StratifiedKFold for cross-validation           â”‚\n",
    "â”‚  â”œâ”€â”€ 3. Predict probabilities (not just classes!)           â”‚\n",
    "â”‚  â””â”€â”€ 4. Calculate ROC-AUC for each fold                     â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  PHASE 3: EVALUATION                                        â”‚\n",
    "â”‚  â”œâ”€â”€ 1. Plot ROC curve for visual inspection                â”‚\n",
    "â”‚  â”œâ”€â”€ 2. Calculate 95% confidence interval (bootstrap)       â”‚\n",
    "â”‚  â”œâ”€â”€ 3. Compare with baseline (random classifier)           â”‚\n",
    "â”‚  â””â”€â”€ 4. Check calibration (reliability diagram)             â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  PHASE 4: INTERPRETATION                                    â”‚\n",
    "â”‚  â”œâ”€â”€ 1. Report mean AUC Â± std across folds                  â”‚\n",
    "â”‚  â”œâ”€â”€ 2. Check for overfitting (train vs val AUC)            â”‚\n",
    "â”‚  â””â”€â”€ 3. Determine optimal threshold for deployment          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ğŸ’» **Complete Code Template**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE ROC-AUC WORKFLOW TEMPLATE\n",
    "print(\"ğŸš€ COMPLETE ROC-AUC WORKFLOW DEMONSTRATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Step 1: Generate realistic data\n",
    "np.random.seed(42)\n",
    "X, y = make_classification(n_samples=2000, n_features=15, n_informative=8, \n",
    "                           n_redundant=3, n_classes=2, weights=[0.7, 0.3], \n",
    "                           flip_y=0.05, random_state=42)\n",
    "\n",
    "print(f\"ğŸ“Š Dataset: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "print(f\"           Class balance: {np.bincount(y)[0]} neg, {np.bincount(y)[1]} pos\")\n",
    "\n",
    "# Step 2: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ‚ï¸  Split: {len(X_train)} train, {len(X_test)} test\")\n",
    "\n",
    "# Step 3: Cross-Validation Setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Step 4: Train Multiple Models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost-like': RandomForestClassifier(n_estimators=200, max_depth=5, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"\\nğŸ¤– TRAINING MODELS WITH CROSS-VALIDATION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nğŸ”¹ {name}:\")\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "    \n",
    "    # Train on full training set\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Test set predictions\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    test_auc = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    # Bootstrap CI for test set\n",
    "    _, lower, upper, _ = bootstrap_auc(y_test, y_proba, n_bootstrap=500)\n",
    "    \n",
    "    results[name] = {\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'test_auc': test_auc,\n",
    "        'ci_lower': lower,\n",
    "        'ci_upper': upper,\n",
    "        'proba': y_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"   CV AUC:  {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
    "    print(f\"   Test AUC: {test_auc:.4f} [{lower:.4f}, {upper:.4f}]\")\n",
    "\n",
    "# Step 5: Comparison and Visualization\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š FINAL COMPARISON TABLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'CV_AUC_Mean': [results[m]['cv_mean'] for m in results],\n",
    "    'CV_AUC_Std': [results[m]['cv_std'] for m in results],\n",
    "    'Test_AUC': [results[m]['test_auc'] for m in results],\n",
    "    'CI_Lower': [results[m]['ci_lower'] for m in results],\n",
    "    'CI_Upper': [results[m]['ci_upper'] for m in results]\n",
    "}).round(4)\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Step 6: ROC Curves Comparison\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "colors = ['blue', 'green', 'red']\n",
    "for (name, data), color in zip(results.items(), colors):\n",
    "    fpr, tpr, _ = roc_curve(y_test, data['proba'])\n",
    "    plt.plot(fpr, tpr, color=color, lw=2, \n",
    "             label=f\"{name} (AUC={data['test_auc']:.3f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves Comparison', fontweight='bold', fontsize=12)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Step 7: Confidence Intervals\n",
    "plt.subplot(1, 2, 2)\n",
    "models_list = list(results.keys())\n",
    "test_aucs = [results[m]['test_auc'] for m in models_list]\n",
    "ci_lower = [results[m]['ci_lower'] for m in models_list]\n",
    "ci_upper = [results[m]['ci_upper'] for m in models_list]\n",
    "\n",
    "errors = [[test_aucs[i] - ci_lower[i] for i in range(len(models_list))],\n",
    "          [ci_upper[i] - test_aucs[i] for i in range(len(models_list))]]\n",
    "\n",
    "x_pos = np.arange(len(models_list))\n",
    "plt.errorbar(x_pos, test_aucs, yerr=errors, fmt='o', capsize=10, capthick=2, \n",
    "             markersize=10, color='darkblue')\n",
    "plt.xticks(x_pos, [m.replace(' ', '\\n') for m in models_list])\n",
    "plt.ylabel('Test AUC')\n",
    "plt.title('Test AUC with 95% Confidence Intervals', fontweight='bold', fontsize=12)\n",
    "plt.ylim(min(ci_lower)-0.05, max(ci_upper)+0.05)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "for i, (model, auc_val) in enumerate(zip(models_list, test_aucs)):\n",
    "    plt.text(i, auc_val, f'{auc_val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('complete_roc_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Step 8: Best Model Selection\n",
    "best_model = max(results, key=lambda x: results[x]['test_auc'])\n",
    "print(f\"\\nğŸ† BEST MODEL: {best_model}\")\n",
    "print(f\"   Test AUC: {results[best_model]['test_auc']:.4f}\")\n",
    "print(f\"   95% CI: [{results[best_model]['ci_lower']:.4f}, {results[best_model]['ci_upper']:.4f}]\")\n",
    "\n",
    "# Step 9: Threshold Analysis (Youden's J statistic)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, results[best_model]['proba'])\n",
    "j_scores = tpr - fpr\n",
    "best_idx = np.argmax(j_scores)\n",
    "optimal_threshold = thresholds[best_idx]\n",
    "\n",
    "print(f\"\\nâš–ï¸  OPTIMAL THRESHOLD (Youden's J): {optimal_threshold:.3f}\")\n",
    "print(f\"   At this threshold: TPR={tpr[best_idx]:.3f}, FPR={fpr[best_idx]:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“– **Chapter 11: Common Pitfalls & How to Avoid Them**\n",
    "\n",
    "### âš ï¸ **Top 10 ROC-AUC Mistakes**\n",
    "\n",
    "| # | Mistake | Why It's Wrong | Solution |\n",
    "|---|---------|----------------|----------|\n",
    "| 1 | **Using accuracy instead of AUC for imbalanced data** | Accuracy can be high by predicting majority class | Always check class balance first |\n",
    "| 2 | **Not using stratified sampling** | Train/test splits might have different class distributions | Use `stratify=y` in train_test_split |\n",
    "| 3 | **Optimizing threshold on test set** | Leads to optimistic performance estimates | Use validation set or CV for threshold tuning |\n",
    "| 4 | **Ignoring calibration** | High AUC doesn't mean probabilities are accurate | Use calibration curves (reliability diagrams) |\n",
    "| 5 | **Comparing AUCs without statistical tests** | Differences might be due to chance | Use DeLong's test or bootstrap CIs |\n",
    "| 6 | **Using AUC for multi-class without modification** | Standard AUC is binary-only | Use OvR (One-vs-Rest) or OvO (One-vs-One) |\n",
    "| 7 | **Not checking for data leakage** | Leaked features inflate AUC artificially | Check feature-target correlations carefully |\n",
    "| 8 | **Reporting AUC without confidence intervals** | Point estimates hide uncertainty | Always provide 95% CIs |\n",
    "| 9 | **Using AUC when you need specific precision/recall** | AUC averages all thresholds | Use F1-score or PR-AUC if specific threshold matters |\n",
    "| 10 | **Forgetting to set random seeds** | Results not reproducible | Always set `random_state` |\n",
    "\n",
    "### ğŸ›¡ï¸ **Defensive Programming Checklist**\n",
    "\n",
    "```python\n",
    "âœ… Before calculating AUC:\n",
    "   â–¡ Check for NaN in predictions\n",
    "   â–¡ Verify predictions are probabilities [0, 1]\n",
    "   â–¡ Confirm both classes present in y_true\n",
    "   â–¡ Check for constant predictions (AUC undefined)\n",
    "   \n",
    "âœ… When comparing models:\n",
    "   â–¡ Use same train/test split\n",
    "   â–¡ Same cross-validation folds\n",
    "   â–¡ Same random seed\n",
    "   â–¡ Statistical significance test\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ **Summary & Cheat Sheet**\n",
    "\n",
    "### ğŸ“ **ROC-AUC Cheat Sheet**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  DEFINITION: Area under the ROC curve (TPR vs FPR)         â”‚\n",
    "â”‚  RANGE: [0.5, 1.0] for useful models (0.5 = random)        â”‚\n",
    "â”‚  INTERPRETATION: Probability that model ranks random        â”‚\n",
    "â”‚                  positive higher than random negative       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ğŸ¯ **When to Use Quick Reference**\n",
    "\n",
    "| Scenario | Use ROC-AUC? | Alternative |\n",
    "|----------|--------------|-------------|\n",
    "| Balanced binary classification | âœ… Yes | - |\n",
    "| Imbalanced (1:10 to 1:100) | âš ï¸ Maybe | Check PR-AUC too |\n",
    "| Imbalanced (> 1:100) | âŒ No | Use PR-AUC |\n",
    "| Need calibrated probabilities | âŒ No | Use Log-loss |\n",
    "| Multi-class classification | âš ï¸ Modify | Use OvR AUC |\n",
    "| Need specific precision/recall | âŒ No | Use F-beta score |\n",
    "\n",
    "### ğŸ“š **Key Formulas**\n",
    "\n",
    "| Metric | Formula | Range |\n",
    "|--------|---------|-------|\n",
    "| TPR (Recall) | TP / (TP + FN) | [0, 1] |\n",
    "| FPR | FP / (FP + TN) | [0, 1] |\n",
    "| AUC | âˆ« TPR d(FPR) | [0, 1] |\n",
    "| Youden's J | TPR - FPR | [-1, 1] |\n",
    "\n",
    "### ğŸ“ **Learning Path**\n",
    "\n",
    "1. âœ… **Beginner:** Understand confusion matrix â†’ TPR/FPR â†’ Basic ROC curve\n",
    "2. âœ… **Intermediate:** Bootstrap CIs â†’ Model comparison â†’ Threshold selection  \n",
    "3. âœ… **Advanced:** PR-AUC â†’ Cost-sensitive learning â†’ Multi-class extensions\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ™ **Thank You!**\n",
    "\n",
    "You've now mastered ROC-AUC from theory to practice! \n",
    "\n",
    "**Next Steps:**\n",
    "- Try this on your own dataset\n",
    "- Experiment with different thresholds\n",
    "- Compare ROC-AUC with PR-AUC\n",
    "- Explore cost-sensitive learning\n",
    "\n",
    "**Remember:** A good data scientist doesn't just calculate metricsâ€”they understand what they mean! ğŸš€\n",
    "\n",
    "---\n",
    "\n",
    "*Happy Modeling!* ğŸ“Šâœ¨\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS: Interactive threshold explorer\n",
    "def interactive_threshold_explorer(y_true, y_proba):\n",
    "    \"\"\"\n",
    "    Interactive widget to explore different thresholds (static version for notebook)\n",
    "    \"\"\"\n",
    "    thresholds = np.arange(0.1, 1.0, 0.1)\n",
    "    metrics = []\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        y_pred = (y_proba >= thresh).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        \n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        f1 = 2 * (precision * tpr) / (precision + tpr) if (precision + tpr) > 0 else 0\n",
    "        \n",
    "        metrics.append({\n",
    "            'Threshold': thresh,\n",
    "            'TPR': tpr,\n",
    "            'FPR': fpr,\n",
    "            'Precision': precision,\n",
    "            'F1': f1\n",
    "        })\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(metrics_df['Threshold'], metrics_df['TPR'], 'g-o', label='TPR (Recall)', linewidth=2)\n",
    "    plt.plot(metrics_df['Threshold'], metrics_df['FPR'], 'r-o', label='FPR', linewidth=2)\n",
    "    plt.plot(metrics_df['Threshold'], metrics_df['Precision'], 'b-o', label='Precision', linewidth=2)\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Metrics vs Threshold', fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(metrics_df['FPR'], metrics_df['TPR'], 'o-', linewidth=2, markersize=8)\n",
    "    for i, thresh in enumerate(thresholds):\n",
    "        plt.annotate(f'{thresh:.1f}', \n",
    "                    (metrics_df.iloc[i]['FPR'], metrics_df.iloc[i]['TPR']),\n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve with Thresholds', fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "# Run interactive explorer\n",
    "print(\"ğŸ® BONUS: Threshold Explorer\")\n",
    "print(\"See how metrics change with different classification thresholds!\\n\")\n",
    "metrics_table = interactive_threshold_explorer(y_test, results[best_model]['proba'])\n",
    "print(\"\\nğŸ“Š Metrics Table:\")\n",
    "print(metrics_table.round(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœï¸ **Author Details**\n",
    "\n",
    "**Name:** Tassawar Abbas  \n",
    "**Email:** [abbas829@gmail.com](mailto:abbas829@gmail.com)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
