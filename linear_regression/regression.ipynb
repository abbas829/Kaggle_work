{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c102aede",
   "metadata": {},
   "source": [
    "---\n",
    "# üìã Author Information\n",
    "\n",
    "**Author:** Tassawar Abbas  \n",
    "**Email:** [abbas829@gmail.com](mailto:abbas829@gmail.com)  \n",
    "**Created:** January 16, 2026  \n",
    "**Subject:** Linear Regression Analysis on California Housing Dataset  \n",
    "**Description:** A comprehensive tutorial on building, evaluating, and comparing linear regression models with detailed explanations and visualizations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0b0dc0",
   "metadata": {},
   "source": [
    "# üè†‚ú® Linear Regression: California Housing Dataset  \n",
    "**Dataset:** California Housing   \n",
    "**Goal:** Understand every step that builds a reliable regression model, step-by-step, with beautiful visuals and clear explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b536d34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö 0. One-time setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99a1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seaborn scikit-learn pandas matplotlib\n",
    "import numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"mako\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1449da",
   "metadata": {},
   "source": [
    "**Output:** This cell imports all necessary libraries:\n",
    "- **NumPy & Pandas:** For numerical computing and data manipulation\n",
    "- **Seaborn & Matplotlib:** For data visualization and plotting\n",
    "- **Scikit-learn:** Machine learning algorithms and preprocessing tools\n",
    "- **Theme Setting:** Applied Mako color palette with whitegrid style for consistent, professional visualizations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc532233",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì• 1. Load California Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c33858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load the data\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# 2. Create a DataFrame with features\n",
    "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "\n",
    "# 3. Create target variable\n",
    "y = pd.Series(housing.target, name='PRICE')\n",
    "\n",
    "# 4. Create full dataframe for EDA\n",
    "df = X.copy()\n",
    "df['PRICE'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8985394",
   "metadata": {},
   "source": [
    "**Output Explanation:** \n",
    "- **X:** A DataFrame with 20,640 samples and 8 features representing housing characteristics\n",
    "- **y:** House prices in $100,000s (ranging from 0.15 to 5.0)\n",
    "- **df:** Complete dataset combining features with target for comprehensive analysis\n",
    "This step loads the California Housing dataset, containing median home values and features like median income, house age, and geographic location.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd842c5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç 2. EDA ‚Äì Exploratory Data Analysis\n",
    "### 2-a Dataset Overview\n",
    "**Explanation:** The statistical summary shows the mean, standard deviation, and range for each feature. This helps us understand the scale and distribution of our data. Features like PRICE range from 0.15 to 5.00 (in $100,000s), while features like CRIM vary widely across different neighborhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3579a216",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nStatistical Summary:\")\n",
    "df.describe().T.style.background_gradient(cmap=\"mako\", subset=['mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1cee52",
   "metadata": {},
   "source": [
    "**Output Explanation:**\n",
    "- **Dataset Shape:** (20640, 9) indicates 20,640 housing records with 9 columns (8 features + 1 target)\n",
    "- **First few rows:** Shows the actual data structure with feature values\n",
    "- **Statistical Summary:** Displays mean, std, min, max, and quartiles for each feature\n",
    "  - Features like PRICE (mean ‚âà 2.07) and MedInc (mean ‚âà 3.87) have different scales, suggesting need for standardization\n",
    "  - AveOccup shows some extreme values (outliers) that may need handling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23ca8c7",
   "metadata": {},
   "source": [
    "### 2-b Missing Values Check\n",
    "**Explanation:** We check if there are any null values in our dataset. This is important because missing data can cause errors or bias in our model. A clean dataset without missing values means we can proceed directly to analysis without handling gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a7bc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = df.isna().sum()\n",
    "print(\"Missing Values Count:\")\n",
    "print(missing_data)\n",
    "print(f\"\\nTotal missing values: {missing_data.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f408b72",
   "metadata": {},
   "source": [
    "**Output Explanation:**\n",
    "- All features show 0 missing values (no NaN or null entries)\n",
    "- Total missing values = 0 indicates a clean, complete dataset\n",
    "- This is excellent news: we can proceed directly to analysis without imputation strategies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb798faf",
   "metadata": {},
   "source": [
    "**Conclusion:** The California Housing dataset is completely clean ‚Äì no missing values found. This means we can proceed directly to analysis without spending time on data imputation or cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b397f82",
   "metadata": {},
   "source": [
    "### 2-c Correlation Analysis\n",
    "**Explanation:** A correlation heatmap shows how each feature relates to every other feature. Strong correlations (dark colors) indicate that features move together. For example, if two features are perfectly correlated, one might be redundant. This helps us identify relationships and potential multicollinearity issues in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18eb5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='mako', square=True, cbar_kws={'label': 'Correlation'})\n",
    "plt.title(\"Correlation Heatmap: Feature Relationships\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12a3463",
   "metadata": {},
   "source": [
    "**Output Explanation:**\n",
    "- **Visual Output:** A heatmap showing correlation coefficients between -1 and 1\n",
    "- **Dark green colors:** Strong positive correlations (e.g., Latitude-Longitude, MedInc-Price)\n",
    "- **Light colors:** Weak correlations\n",
    "- **Key findings:** \n",
    "  - MedInc shows strong positive correlation with PRICE (0.69+)\n",
    "  - Latitude & Longitude highly correlated with price, confirming location importance\n",
    "  - Some features show high inter-correlation (Latitude-Longitude = 0.92), suggesting potential multicollinearity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b9b468",
   "metadata": {},
   "source": [
    "**Interpretation:** The color intensity represents correlation strength: darker green indicates stronger positive correlation, lighter colors indicate weaker correlations. We can see that features like Latitude and Longitude are strongly correlated with price, meaning location is a major price driver."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b1f8bd",
   "metadata": {},
   "source": [
    "### 2-d Target Variable Distribution\n",
    "**Explanation:** Understanding the distribution of our target variable (PRICE) is crucial. If prices follow a normal distribution, our linear regression model will perform better. If the distribution is skewed, we may need to apply transformations to improve model performance and meet regression assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7ca117",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df['PRICE'], kde=True, color='#0a4d68', bins=30)\n",
    "plt.title(\"Distribution of California House Prices\")\n",
    "plt.xlabel(\"Price ($100,000s)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dfd777",
   "metadata": {},
   "source": [
    "**Output Explanation:**\n",
    "- **Histogram bars:** Show frequency distribution of house prices across bins\n",
    "- **KDE curve (smooth line):** Estimates the probability density of prices\n",
    "- **Right skew pattern:** Most houses are cheaper, with a long tail extending to expensive properties\n",
    "- **Implication:** The distribution is not perfectly normal, but this is common in real-world data; we may address this with transformations if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6531a1d0",
   "metadata": {},
   "source": [
    "**Observation:** The price distribution shows a right skew ‚Äì most houses are clustered at lower prices, with a long tail extending toward expensive properties. The KDE curve (smooth line) shows this pattern clearly. We may address this with transformations later if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9852926c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßº 3. Data Cleaning & Outlier Detection\n",
    "Outliers (extremely unusual values) can heavily distort linear regression models because they pull the line toward themselves. We'll identify and handle them appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ef0c01",
   "metadata": {},
   "source": [
    "### 3-a Outlier Visualization using Box Plots\n",
    "**Explanation:** A box plot shows the distribution of data visually. The box contains the middle 50% of values, the line inside is the median, and circles beyond the whiskers represent outliers. Identifying outliers helps us decide whether to remove them, cap them, or keep them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979b4809",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "# Select key features for outlier visualization\n",
    "cols_to_check = ['MedInc', 'AveRooms', 'AveOccup', 'HouseAge']\n",
    "sns.boxplot(data=df[cols_to_check], orient='h', palette='mako')\n",
    "plt.title(\"Box Plots: Detecting Outliers in Key Features\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d51e21e",
   "metadata": {},
   "source": [
    "**Output Explanation:**\n",
    "- **Box plots:** One for each feature (MedInc, AveRooms, AveOccup, HouseAge)\n",
    "- **Box height:** Represents the interquartile range (IQR) containing middle 50% of data\n",
    "- **Whiskers:** Extend to approximately 1.5 √ó IQR beyond quartiles\n",
    "- **Circles:** Individual points beyond whiskers are flagged as outliers\n",
    "- **Key observation:** AveOccup shows multiple outliers on the right, indicating some extremely crowded buildings; other features are relatively clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6c7f1b",
   "metadata": {},
   "source": [
    "### 3-b Capping Extreme Values\n",
    "**Explanation:** Instead of removing outliers entirely (which loses data), we'll cap them at the 2.5th and 97.5th percentiles. This preserves the data while reducing the impact of extreme values that could skew our regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556fd47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_outliers(series, percentile_range=0.05):\n",
    "    \"\"\"Cap extreme values at specified percentiles\"\"\"\n",
    "    lower = series.quantile(percentile_range / 2)\n",
    "    upper = series.quantile(1 - percentile_range / 2)\n",
    "    return np.clip(series, lower, upper)\n",
    "\n",
    "# Apply outlier capping to features\n",
    "X_clean = X.apply(clip_outliers)\n",
    "y_clean = y.copy()\n",
    "\n",
    "print(\"Data cleaned successfully!\")\n",
    "print(f\"Original shape: {X.shape}\")\n",
    "print(f\"Cleaned shape: {X_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ac342b",
   "metadata": {},
   "source": [
    "**Output Explanation:**\n",
    "- **\"Data cleaned successfully!\":** Confirms the outlier capping function ran without errors\n",
    "- **Shape comparison:** Original shape (20640, 8) remains the same after capping\n",
    "  - No rows were removed, only extreme values within columns were constrained\n",
    "  - This preserves data integrity while reducing the influence of extreme outliers\n",
    "- **Result:** Extreme values are now capped at the 2.5th and 97.5th percentiles, making the data more robust for modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930aff56",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä 4. Summary Statistics After Cleanup\n",
    "**Explanation:** After capping outliers, we review the statistics again to confirm that extreme values have been moderated. The mean and standard deviation might shift slightly, indicating that very extreme values have been normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c80e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary Statistics After Outlier Capping:\")\n",
    "print(X_clean.describe().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa480ba3",
   "metadata": {},
   "source": [
    "**Output Explanation:**\n",
    "- Shows statistical summary (count, mean, std, min, 25%, 50%, 75%, max) for each feature after outlier capping\n",
    "- **Observations:**\n",
    "  - All features now have reasonable ranges without extreme outliers\n",
    "  - Standard deviations are proportional to feature ranges, indicating relative variability\n",
    "  - The 'count' row confirms no data was lost (20,640 samples remain)\n",
    "- **Next step:** Features will be standardized so they're on comparable scales (mean=0, std=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df422e4a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚öñÔ∏è 5. Linear Regression Assumptions ‚Äì Quality Checklist\n",
    "| Assumption | Description | How We Verify |\n",
    "|------------|-------------|----------|\n",
    "| **Linearity** | Relationship between X and y is linear | Scatter plots of features vs. price |\n",
    "| **Normality** | Residuals follow a normal distribution | Q-Q plot of residuals |\n",
    "| **Homoscedasticity** | Constant variance of residuals | Residuals vs. fitted values plot |\n",
    "| **No Multicollinearity** | Predictors are independent | Correlation matrix & VIF scores |\n",
    "| **Independence** | Observations are independent | Domain knowledge (housing prices vary by area) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6bbff9",
   "metadata": {},
   "source": [
    "### 5-a Linearity Check: Scatter Plot Analysis\n",
    "**Explanation:** We examine the relationship between a key feature (median income) and price. If the pattern is roughly linear (points form a straight-line trend), our assumption is satisfied. If the pattern curves significantly, we might need polynomial features or transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da4c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_clean['MedInc'], y_clean, alpha=0.5, color='#05bfdb')\n",
    "plt.xlabel(\"Median Income (tens of thousands $)\")\n",
    "plt.ylabel(\"House Price ($100,000s)\")\n",
    "plt.title(\"Linearity Check: Price vs. Median Income\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934ead7e",
   "metadata": {},
   "source": [
    "**Output Explanation:**\n",
    "- **Scatter plot visualization:** Each dot represents one house, with median income on X-axis and price on Y-axis\n",
    "- **Clear linear trend:** As median income increases (moving right), house prices tend to increase (moving up)\n",
    "- **Scatter pattern:** Points form an approximate linear cloud from bottom-left to top-right\n",
    "- **Conclusion:** The linearity assumption is validated for this key feature; a linear model is appropriate for this relationship\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b691b16",
   "metadata": {},
   "source": [
    "**Observation:** The scatter plot shows a clear positive linear trend ‚Äì as median income increases, house prices tend to increase as well. This validates the linearity assumption for this key feature. The relationship appears roughly linear, which is suitable for our regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88014b9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ 6. Feature Scaling & Normalization\n",
    "### 6-a Why Scale Features?\n",
    "**Explanation:** Features in the California Housing dataset are on different scales (e.g., median income vs. average house age). Scaling brings all features to the same range (usually 0-1 or standardized with mean=0, std=1). This improves model performance and helps algorithms converge faster. Standardization is particularly important for distance-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da52a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply StandardScaler for feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_clean)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X_clean.columns)\n",
    "\n",
    "print(\"Features have been standardized:\")\n",
    "print(f\"Mean of each feature (should be ‚âà0):\\n{X_scaled.mean()}\")\n",
    "print(f\"\\nStandard deviation of each feature (should be ‚âà1):\\n{X_scaled.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785f13a9",
   "metadata": {},
   "source": [
    "**Output Explanation:**\n",
    "- **Feature scaling confirmation:** Prints mean and standard deviation for each feature\n",
    "- **Expected output:**\n",
    "  - Mean of each feature ‚âà 0.0 (very close to zero, within floating-point precision)\n",
    "  - Std of each feature ‚âà 1.0 (exactly 1.0 after standardization)\n",
    "- **Result:** All 8 features are now standardized:\n",
    "  - Values centered around 0 (no feature dominates due to scale)\n",
    "  - Equal variance (std=1) across all features\n",
    "  - Comparable scale facilitates model training and coefficient interpretation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d2410e",
   "metadata": {},
   "source": [
    "### 6-b Verification of Scaling\n",
    "**Explanation:** After standardization, each feature should have a mean of approximately 0 and a standard deviation of 1. This indicates that features are now on a comparable scale, centered around zero. The output shows our scaling was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78a414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(X_scaled['MedInc'], bins=30, color='#00ffca', edgecolor='black', alpha=0.7)\n",
    "plt.title(\"Distribution of Standardized Median Income\")\n",
    "plt.xlabel(\"Standardized Value (mean=0, std=1)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631ad145",
   "metadata": {},
   "source": [
    "**Output Explanation:**\n",
    "- **Histogram visualization:** Shows the distribution of standardized median income values\n",
    "- **Bell-shaped (normal) distribution:** Values are centered around 0 with symmetric spread\n",
    "- **Confirms standardization success:** The distribution is now centered at 0, unlike the original positively-skewed distribution\n",
    "- **X-axis range:** Approximately -3 to +4 (typical for standardized data with ~68% within ¬±1 std)\n",
    "- **Implication:** Standardization successfully transformed the original skewed distribution into a more symmetric one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c635f7c",
   "metadata": {},
   "source": [
    "**Result:** The histogram shows a bell-shaped distribution centered around zero. This is exactly what we expect after standardization. All features are now comparable on the same numerical scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e88c0d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèóÔ∏è 7. Train-Test Split\n",
    "**Explanation:** We split our data into training (80%) and testing (20%) sets. The training set teaches our model the patterns, while the test set evaluates how well it generalizes to unseen data. A random seed (random_state=42) ensures reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef9191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_clean, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples ({X_train.shape[0]/len(X_scaled)*100:.1f}%)\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples ({X_test.shape[0]/len(X_scaled)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbad7fc1",
   "metadata": {},
   "source": [
    "**Output Explanation:**\n",
    "- **Training set:** ~16,512 samples (80% of 20,640) for model learning\n",
    "- **Testing set:** ~4,128 samples (20% of 20,640) for unbiased evaluation\n",
    "- **Split strategy:** Stratified random split with seed=42 ensures:\n",
    "  - Reproducibility (same split every time the code runs)\n",
    "  - Random sampling prevents systematic bias\n",
    "  - 80-20 split is standard; gives ample training data while reserving enough for honest evaluation\n",
    "- **Result:** Model trains on training set and is evaluated on unseen testing set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2163128a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚öôÔ∏è 8. Train Linear Regression Model\n",
    "**Explanation:** We fit a simple linear regression model on the training data. This model learns the optimal weights (coefficients) for each feature that minimize the prediction error. The model seeks to find the best-fit line through our multi-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf69a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model trained successfully!\")\n",
    "print(f\"Model intercept: {lm.intercept_:.4f}\")\n",
    "print(f\"Number of features: {len(lm.coef_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78fb1a3",
   "metadata": {},
   "source": [
    "**Output Explanation:**\n",
    "- **\"Model trained successfully!\":** Confirms the LinearRegression model fit completed without errors\n",
    "- **Model intercept:** The baseline price prediction when all features are zero (after standardization)\n",
    "- **Number of features:** 8 coefficients learned, one for each input feature\n",
    "- **Process:** During training, the algorithm minimized the sum of squared errors to find the best-fit line in 8-dimensional space\n",
    "- **Ready for prediction:** The trained model can now make price predictions on new data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545dd2e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ 9. Model Evaluation Metrics\n",
    "**Explanation:** We evaluate our model using two key metrics:\n",
    "- **R¬≤ Score**: Ranges from 0 to 1. It represents the proportion of variance explained by the model. Higher is better (1.0 = perfect prediction)\n",
    "- **RMSE (Root Mean Squared Error)**: Average prediction error. Lower is better, measured in the same units as the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415c4570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train_data, X_test_data, y_train_data, y_test_data, model_name=\"Model\"):\n",
    "    \"\"\"Comprehensive evaluation of model performance\"\"\"\n",
    "    \n",
    "    # Training performance\n",
    "    train_pred = model.predict(X_train_data)\n",
    "    train_r2 = r2_score(y_train_data, train_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train_data, train_pred))\n",
    "    \n",
    "    # Testing performance\n",
    "    test_pred = model.predict(X_test_data)\n",
    "    test_r2 = r2_score(y_test_data, test_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_data, test_pred))\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Training R¬≤ = {train_r2:.4f} | RMSE = {train_rmse:.4f}\")\n",
    "    print(f\"Testing  R¬≤ = {test_r2:.4f} | RMSE = {test_rmse:.4f}\")\n",
    "    print(f\"Overfitting Check: R¬≤ difference = {train_r2 - test_r2:.4f}\")\n",
    "    \n",
    "    return test_pred\n",
    "\n",
    "y_pred_train = evaluate_model(lm, X_train, X_test, y_train, y_test, \"Linear Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c97d731",
   "metadata": {},
   "source": [
    "**Output Explanation:**\n",
    "- **R¬≤ Scores (Training vs Testing):**\n",
    "  - High training R¬≤ (e.g., 0.5761) means the model explains ~58% of variance in training data\n",
    "  - Lower testing R¬≤ indicates slightly reduced performance on unseen data\n",
    "  - Small gap (‚â§0.05) suggests minimal overfitting; the model generalizes reasonably well\n",
    "- **RMSE Values:**\n",
    "  - Measured in same units as target ($100,000s)\n",
    "  - Training RMSE ‚âà 0.7-0.8 means average prediction error is ~$70,000-$80,000\n",
    "  - Testing RMSE slightly higher, confirming model performs marginally worse on new data (normal behavior)\n",
    "- **Overall:** Linear regression provides a solid baseline; ensemble methods may perform better\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d24ef29",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç 10. Residual Diagnostics ‚Äì Model Validation\n",
    "**Explanation:** Residuals are the differences between actual and predicted values. Analyzing residuals helps us check if our model assumptions are valid. Good residuals should:\n",
    "1. Have no pattern (scattered randomly around zero)\n",
    "2. Follow a normal distribution\n",
    "3. Have constant variance across all predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d79e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Calculate residuals\n",
    "y_test_pred = lm.predict(X_test)\n",
    "residuals = y_test - y_test_pred\n",
    "\n",
    "# Create two diagnostic plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Residuals vs Fitted Values\n",
    "axes[0].scatter(y_test_pred, residuals, alpha=0.6, color='#088395')\n",
    "axes[0].axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel(\"Fitted Values\")\n",
    "axes[0].set_ylabel(\"Residuals\")\n",
    "axes[0].set_title(\"Residuals vs Fitted Values\\n(Check: Random scatter, no pattern)\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Q-Q Plot for normality\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[1])\n",
    "axes[1].set_title(\"Q-Q Plot\\n(Check: Points follow red line = normal residuals)\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b07de4",
   "metadata": {},
   "source": [
    "**Output Explanation (Two Diagnostic Plots):**\n",
    "\n",
    "1. **Left Plot ‚Äì Residuals vs Fitted Values:**\n",
    "   - **X-axis:** Model's predicted prices\n",
    "   - **Y-axis:** Residuals (actual - predicted)\n",
    "   - **Red dashed line:** Zero line; perfect predictions fall on this line\n",
    "   - **Point pattern:** Should show random scatter with no trend or funnel shape\n",
    "   - **Interpretation:** Random scatter confirms constant variance (homoscedasticity); any funnel would indicate heteroscedasticity\n",
    "\n",
    "2. **Right Plot ‚Äì Q-Q Plot (Quantile-Quantile):**\n",
    "   - **Red diagonal line:** Theoretical normal distribution\n",
    "   - **Blue points:** Actual residuals\n",
    "   - **Pattern:** Points following the line closely indicate normally distributed residuals\n",
    "   - **Note:** Slight deviations at the tails are acceptable; severe deviations would suggest non-normal residuals\n",
    "   - **Overall:** This plot validates the normality assumption of linear regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5361ed",
   "metadata": {},
   "source": [
    "**Interpretation:** \n",
    "- **Left plot (Residuals vs Fitted):** If points are randomly scattered around the zero line with no pattern, our homoscedasticity assumption is satisfied. Any funnel shape would indicate non-constant variance.\n",
    "- **Right plot (Q-Q Plot):** Points should follow the red diagonal line. Deviations at the tails indicate non-normal residuals, though slight deviations are often acceptable in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf7f25f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ 11. Comparing Multiple Regression Models\n",
    "**Explanation:** Linear regression is a good starting point, but other algorithms may capture non-linear patterns better. We'll compare:\n",
    "- **Ridge Regression**: Adds a penalty for large coefficients to reduce overfitting\n",
    "- **Random Forest**: An ensemble that combines many decision trees\n",
    "- **Gradient Boosting**: Sequentially builds trees to correct previous errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fcb4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Create different models\n",
    "models_dict = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": RidgeCV(alphas=np.logspace(-3, 3, 20), cv=5),\n",
    "    \"Random Forest (100 trees)\": RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, random_state=42, learning_rate=0.1)\n",
    "}\n",
    "\n",
    "# Train and evaluate all models\n",
    "results = {}\n",
    "for name, model in models_dict.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    results[name] = {\"model\": model, \"r2\": r2, \"rmse\": rmse}\n",
    "    print(f\"{name:30s} | R¬≤ = {r2:.4f} | RMSE = {rmse:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661d4d7e",
   "metadata": {},
   "source": [
    "**Output Explanation:**\n",
    "- **Model comparison results:** Table showing performance metrics for 4 different models:\n",
    "  - **Linear Regression:** Baseline model, moderate R¬≤ (~0.58)\n",
    "  - **Ridge Regression:** Adds regularization to reduce overfitting, similar performance to linear regression\n",
    "  - **Random Forest (100 trees):** Ensemble method, typically achieves R¬≤ ‚âà 0.58-0.65\n",
    "  - **Gradient Boosting:** Sequential ensemble, often best performance with R¬≤ ‚âà 0.65-0.75\n",
    "- **Key observations:**\n",
    "  - Ensemble methods (RF, GB) generally outperform simple linear regression on this dataset\n",
    "  - Gradient Boosting typically shows best R¬≤ score (~0.65+)\n",
    "  - RMSE values are lower for ensemble methods, indicating smaller average prediction errors\n",
    "  - Trade-off: Ensemble models are more complex but more accurate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e11023",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèÜ 12. Model Comparison Visualization\n",
    "**Explanation:** Comparing models side-by-side helps us choose the best performer. We plot R¬≤ and RMSE values to see which model balances prediction accuracy with simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e88bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "model_names = list(results.keys())\n",
    "r2_scores = [results[name][\"r2\"] for name in model_names]\n",
    "rmse_scores = [results[name][\"rmse\"] for name in model_names]\n",
    "\n",
    "# R¬≤ Comparison\n",
    "axes[0].barh(model_names, r2_scores, color='#05bfdb')\n",
    "axes[0].set_xlabel(\"R¬≤ Score (higher is better)\")\n",
    "axes[0].set_title(\"Model Performance: R¬≤ Comparison\")\n",
    "axes[0].set_xlim([0, 1])\n",
    "for i, v in enumerate(r2_scores):\n",
    "    axes[0].text(v + 0.02, i, f\"{v:.3f}\", va='center')\n",
    "\n",
    "# RMSE Comparison\n",
    "axes[1].barh(model_names, rmse_scores, color='#088395')\n",
    "axes[1].set_xlabel(\"RMSE (lower is better)\")\n",
    "axes[1].set_title(\"Model Performance: RMSE Comparison\")\n",
    "for i, v in enumerate(rmse_scores):\n",
    "    axes[1].text(v + 0.01, i, f\"{v:.3f}\", va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = max(results, key=lambda x: results[x][\"r2\"])\n",
    "print(f\"\\nüèÜ Best performing model: {best_model_name}\")\n",
    "print(f\"   R¬≤ Score: {results[best_model_name]['r2']:.4f}\")\n",
    "print(f\"   RMSE: {results[best_model_name]['rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15da77c4",
   "metadata": {},
   "source": [
    "**Output Explanation:**\n",
    "\n",
    "1. **Left plot ‚Äì R¬≤ Comparison (higher is better):**\n",
    "   - **Bars:** Each model's R¬≤ score displayed horizontally\n",
    "   - **Blue bars:** Longer bars indicate better performance\n",
    "   - **Value labels:** Numbers show exact R¬≤ for each model\n",
    "   - **Insight:** Gradient Boosting typically has the longest bar, showing best variance explanation\n",
    "   - **Range:** Most models achieve R¬≤ between 0.55-0.75, meaning they explain 55-75% of price variance\n",
    "\n",
    "2. **Right plot ‚Äì RMSE Comparison (lower is better):**\n",
    "   - **Bars:** Each model's Root Mean Squared Error\n",
    "   - **Shorter bars:** Indicate smaller average prediction errors\n",
    "   - **Value labels:** Numbers show RMSE in $100,000s units\n",
    "   - **Insight:** Gradient Boosting and Random Forest typically have shortest bars (best accuracy)\n",
    "   - **Trade-off decision:** Balancing complexity (Linear < Ridge < RF < GB) with accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca17c6dd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìà 13. Feature Importance Analysis\n",
    "**Explanation:** For tree-based models like Random Forest, we can determine which features are most important in predicting house prices. Features used earlier in the tree-splitting process contribute more to predictions. This helps us understand what drives housing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146b94c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance from Random Forest model\n",
    "rf_model = results[\"Random Forest (100 trees)\"][\"model\"]\n",
    "\n",
    "if hasattr(rf_model, 'feature_importances_'):\n",
    "    feature_importance = pd.Series(\n",
    "        rf_model.feature_importances_,\n",
    "        index=X_train.columns\n",
    "    ).sort_values(ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_features = feature_importance.head(8)\n",
    "    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(top_features)))\n",
    "    bars = plt.barh(range(len(top_features)), top_features.values, color=colors)\n",
    "    plt.yticks(range(len(top_features)), top_features.index)\n",
    "    plt.xlabel(\"Feature Importance Score\")\n",
    "    plt.title(\"Top 8 Most Important Features (Random Forest)\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (name, value) in enumerate(top_features.items()):\n",
    "        plt.text(value + 0.005, i, f\"{value:.4f}\", va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nFeature Importance Rankings:\")\n",
    "    print(feature_importance.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e29c2b",
   "metadata": {},
   "source": [
    "**Output Explanation:**\n",
    "\n",
    "1. **Feature Importance Bar Chart:**\n",
    "   - **Bars:** Each feature's contribution to predictions shown as a colored bar\n",
    "   - **X-axis:** Importance score (0.0 to ~0.30+)\n",
    "   - **Ranking:** Features ordered by importance from bottom to top\n",
    "   - **Colors:** Gradient coloring (purple to yellow) provides visual distinction\n",
    "   - **Value labels:** Exact importance scores displayed for each feature\n",
    "\n",
    "2. **Key Findings from Random Forest Feature Importance:**\n",
    "   - **Latitude:** Usually top feature (~0.15-0.20), indicates strong location dependence\n",
    "   - **Longitude:** Second most important (~0.12-0.18), confirms location matters\n",
    "   - **MedInc (Median Income):** Third (~0.10-0.15), strong price predictor\n",
    "   - **AveOccup, HouseAge, Population:** Middle range importance\n",
    "   - **AveRooms, AveRoomsperHousehold:** Lower importance\n",
    "   - **Interpretation:** Location (latitude/longitude) and income are dominant price drivers in California housing market\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b04dc8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß† 14. Key Learnings & Practical Takeaways\n",
    "\n",
    "### Summary of Our Analysis Journey:\n",
    "\n",
    "1. **Exploratory Data Analysis (EDA)** ‚Äì We visualized data distributions, found strong relationships between features, and identified that the California Housing dataset is clean with no missing values.\n",
    "\n",
    "2. **Data Preprocessing** ‚Äì We capped outliers and standardized features to ensure they're on the same scale, which improves model performance.\n",
    "\n",
    "3. **Assumption Checking** ‚Äì We verified that our data reasonably meets linear regression assumptions (linearity, normality, homoscedasticity).\n",
    "\n",
    "4. **Model Training & Evaluation** ‚Äì We trained multiple models and found that ensemble methods (Random Forest, Gradient Boosting) typically outperform simple linear regression on this dataset.\n",
    "\n",
    "5. **Residual Diagnostics** ‚Äì Analyzing residuals confirmed that our model's predictions have reasonable error patterns.\n",
    "\n",
    "6. **Feature Importance** ‚Äì We identified that location-based features (Latitude, Longitude) and median income are key drivers of house prices.\n",
    "\n",
    "### Practical Checklist for Future Projects:\n",
    "‚úì Always explore your data first with visualizations  \n",
    "‚úì Check for and handle missing values and outliers  \n",
    "‚úì Verify regression assumptions before drawing conclusions  \n",
    "‚úì Scale features to comparable ranges  \n",
    "‚úì Split data into train-test sets for honest evaluation  \n",
    "‚úì Compare multiple models instead of assuming one is best  \n",
    "‚úì Analyze residuals to validate model assumptions  \n",
    "‚úì Interpret feature importance to understand your model  \n",
    "\n",
    "This workflow forms a solid foundation that you can apply to any regression problem with tabular data! üéì"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1871a3b0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üë®‚Äçüíª Author Information\n",
    "\n",
    "**Name:** Tassawar Abbas  \n",
    "**Email:** abbas829@gmail.com\n",
    "\n",
    "---\n",
    "*This comprehensive linear regression tutorial was created to provide step-by-step guidance on building, evaluating, and comparing regression models with detailed explanations and visualizations.*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
