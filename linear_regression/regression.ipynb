{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c102aede",
   "metadata": {},
   "source": [
    "---\n",
    "# üìã Author Information\n",
    "\n",
    "**Author:** Tassawar Abbas  \n",
    "**Email:** [abbas829@gmail.com](mailto:abbas829@gmail.com)  \n",
    "**Created:** January 16, 2026  \n",
    "**Subject:** Linear Regression Analysis on California Housing Dataset  \n",
    "**Description:** A comprehensive tutorial on building, evaluating, and comparing linear regression models with detailed explanations and visualizations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0b0dc0",
   "metadata": {},
   "source": [
    "# üè†‚ú® Linear Regression: California Housing Dataset  \n",
    "**Dataset:** California Housing   \n",
    "**Goal:** Understand every step that builds a reliable regression model, step-by-step, with beautiful visuals and clear explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b536d34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö 0. One-time setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99a1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seaborn scikit-learn pandas matplotlib\n",
    "import numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"mako\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc532233",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì• 1. Load California Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c33858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load the data\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# 2. Create a DataFrame with features\n",
    "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "\n",
    "# 3. Create target variable\n",
    "y = pd.Series(housing.target, name='PRICE')\n",
    "\n",
    "# 4. Create full dataframe for EDA\n",
    "df = X.copy()\n",
    "df['PRICE'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd842c5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç 2. EDA ‚Äì Exploratory Data Analysis\n",
    "### 2-a Dataset Overview\n",
    "**Explanation:** The statistical summary shows the mean, standard deviation, and range for each feature. This helps us understand the scale and distribution of our data. Features like PRICE range from 0.15 to 5.00 (in $100,000s), while features like CRIM vary widely across different neighborhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3579a216",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nStatistical Summary:\")\n",
    "df.describe().T.style.background_gradient(cmap=\"mako\", subset=['mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23ca8c7",
   "metadata": {},
   "source": [
    "### 2-b Missing Values Check\n",
    "**Explanation:** We check if there are any null values in our dataset. This is important because missing data can cause errors or bias in our model. A clean dataset without missing values means we can proceed directly to analysis without handling gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a7bc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = df.isna().sum()\n",
    "print(\"Missing Values Count:\")\n",
    "print(missing_data)\n",
    "print(f\"\\nTotal missing values: {missing_data.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb798faf",
   "metadata": {},
   "source": [
    "**Conclusion:** The California Housing dataset is completely clean ‚Äì no missing values found. This means we can proceed directly to analysis without spending time on data imputation or cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b397f82",
   "metadata": {},
   "source": [
    "### 2-c Correlation Analysis\n",
    "**Explanation:** A correlation heatmap shows how each feature relates to every other feature. Strong correlations (dark colors) indicate that features move together. For example, if two features are perfectly correlated, one might be redundant. This helps us identify relationships and potential multicollinearity issues in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18eb5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='mako', square=True, cbar_kws={'label': 'Correlation'})\n",
    "plt.title(\"Correlation Heatmap: Feature Relationships\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b9b468",
   "metadata": {},
   "source": [
    "**Interpretation:** The color intensity represents correlation strength: darker green indicates stronger positive correlation, lighter colors indicate weaker correlations. We can see that features like Latitude and Longitude are strongly correlated with price, meaning location is a major price driver."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b1f8bd",
   "metadata": {},
   "source": [
    "### 2-d Target Variable Distribution\n",
    "**Explanation:** Understanding the distribution of our target variable (PRICE) is crucial. If prices follow a normal distribution, our linear regression model will perform better. If the distribution is skewed, we may need to apply transformations to improve model performance and meet regression assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7ca117",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df['PRICE'], kde=True, color='#0a4d68', bins=30)\n",
    "plt.title(\"Distribution of California House Prices\")\n",
    "plt.xlabel(\"Price ($100,000s)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6531a1d0",
   "metadata": {},
   "source": [
    "**Observation:** The price distribution shows a right skew ‚Äì most houses are clustered at lower prices, with a long tail extending toward expensive properties. The KDE curve (smooth line) shows this pattern clearly. We may address this with transformations later if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9852926c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßº 3. Data Cleaning & Outlier Detection\n",
    "Outliers (extremely unusual values) can heavily distort linear regression models because they pull the line toward themselves. We'll identify and handle them appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ef0c01",
   "metadata": {},
   "source": [
    "### 3-a Outlier Visualization using Box Plots\n",
    "**Explanation:** A box plot shows the distribution of data visually. The box contains the middle 50% of values, the line inside is the median, and circles beyond the whiskers represent outliers. Identifying outliers helps us decide whether to remove them, cap them, or keep them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979b4809",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "# Select key features for outlier visualization\n",
    "cols_to_check = ['MedInc', 'AveRooms', 'AveOccup', 'HouseAge']\n",
    "sns.boxplot(data=df[cols_to_check], orient='h', palette='mako')\n",
    "plt.title(\"Box Plots: Detecting Outliers in Key Features\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6c7f1b",
   "metadata": {},
   "source": [
    "### 3-b Capping Extreme Values\n",
    "**Explanation:** Instead of removing outliers entirely (which loses data), we'll cap them at the 2.5th and 97.5th percentiles. This preserves the data while reducing the impact of extreme values that could skew our regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556fd47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_outliers(series, percentile_range=0.05):\n",
    "    \"\"\"Cap extreme values at specified percentiles\"\"\"\n",
    "    lower = series.quantile(percentile_range / 2)\n",
    "    upper = series.quantile(1 - percentile_range / 2)\n",
    "    return np.clip(series, lower, upper)\n",
    "\n",
    "# Apply outlier capping to features\n",
    "X_clean = X.apply(clip_outliers)\n",
    "y_clean = y.copy()\n",
    "\n",
    "print(\"Data cleaned successfully!\")\n",
    "print(f\"Original shape: {X.shape}\")\n",
    "print(f\"Cleaned shape: {X_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930aff56",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä 4. Summary Statistics After Cleanup\n",
    "**Explanation:** After capping outliers, we review the statistics again to confirm that extreme values have been moderated. The mean and standard deviation might shift slightly, indicating that very extreme values have been normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c80e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary Statistics After Outlier Capping:\")\n",
    "print(X_clean.describe().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df422e4a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚öñÔ∏è 5. Linear Regression Assumptions ‚Äì Quality Checklist\n",
    "| Assumption | Description | How We Verify |\n",
    "|------------|-------------|----------|\n",
    "| **Linearity** | Relationship between X and y is linear | Scatter plots of features vs. price |\n",
    "| **Normality** | Residuals follow a normal distribution | Q-Q plot of residuals |\n",
    "| **Homoscedasticity** | Constant variance of residuals | Residuals vs. fitted values plot |\n",
    "| **No Multicollinearity** | Predictors are independent | Correlation matrix & VIF scores |\n",
    "| **Independence** | Observations are independent | Domain knowledge (housing prices vary by area) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6bbff9",
   "metadata": {},
   "source": [
    "### 5-a Linearity Check: Scatter Plot Analysis\n",
    "**Explanation:** We examine the relationship between a key feature (median income) and price. If the pattern is roughly linear (points form a straight-line trend), our assumption is satisfied. If the pattern curves significantly, we might need polynomial features or transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da4c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_clean['MedInc'], y_clean, alpha=0.5, color='#05bfdb')\n",
    "plt.xlabel(\"Median Income (tens of thousands $)\")\n",
    "plt.ylabel(\"House Price ($100,000s)\")\n",
    "plt.title(\"Linearity Check: Price vs. Median Income\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b691b16",
   "metadata": {},
   "source": [
    "**Observation:** The scatter plot shows a clear positive linear trend ‚Äì as median income increases, house prices tend to increase as well. This validates the linearity assumption for this key feature. The relationship appears roughly linear, which is suitable for our regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88014b9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ 6. Feature Scaling & Normalization\n",
    "### 6-a Why Scale Features?\n",
    "**Explanation:** Features in the California Housing dataset are on different scales (e.g., median income vs. average house age). Scaling brings all features to the same range (usually 0-1 or standardized with mean=0, std=1). This improves model performance and helps algorithms converge faster. Standardization is particularly important for distance-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da52a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply StandardScaler for feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_clean)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X_clean.columns)\n",
    "\n",
    "print(\"Features have been standardized:\")\n",
    "print(f\"Mean of each feature (should be ‚âà0):\\n{X_scaled.mean()}\")\n",
    "print(f\"\\nStandard deviation of each feature (should be ‚âà1):\\n{X_scaled.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d2410e",
   "metadata": {},
   "source": [
    "### 6-b Verification of Scaling\n",
    "**Explanation:** After standardization, each feature should have a mean of approximately 0 and a standard deviation of 1. This indicates that features are now on a comparable scale, centered around zero. The output shows our scaling was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78a414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(X_scaled['MedInc'], bins=30, color='#00ffca', edgecolor='black', alpha=0.7)\n",
    "plt.title(\"Distribution of Standardized Median Income\")\n",
    "plt.xlabel(\"Standardized Value (mean=0, std=1)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c635f7c",
   "metadata": {},
   "source": [
    "**Result:** The histogram shows a bell-shaped distribution centered around zero. This is exactly what we expect after standardization. All features are now comparable on the same numerical scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e88c0d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèóÔ∏è 7. Train-Test Split\n",
    "**Explanation:** We split our data into training (80%) and testing (20%) sets. The training set teaches our model the patterns, while the test set evaluates how well it generalizes to unseen data. A random seed (random_state=42) ensures reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef9191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_clean, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples ({X_train.shape[0]/len(X_scaled)*100:.1f}%)\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples ({X_test.shape[0]/len(X_scaled)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2163128a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚öôÔ∏è 8. Train Linear Regression Model\n",
    "**Explanation:** We fit a simple linear regression model on the training data. This model learns the optimal weights (coefficients) for each feature that minimize the prediction error. The model seeks to find the best-fit line through our multi-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf69a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model trained successfully!\")\n",
    "print(f\"Model intercept: {lm.intercept_:.4f}\")\n",
    "print(f\"Number of features: {len(lm.coef_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545dd2e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ 9. Model Evaluation Metrics\n",
    "**Explanation:** We evaluate our model using two key metrics:\n",
    "- **R¬≤ Score**: Ranges from 0 to 1. It represents the proportion of variance explained by the model. Higher is better (1.0 = perfect prediction)\n",
    "- **RMSE (Root Mean Squared Error)**: Average prediction error. Lower is better, measured in the same units as the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415c4570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train_data, X_test_data, y_train_data, y_test_data, model_name=\"Model\"):\n",
    "    \"\"\"Comprehensive evaluation of model performance\"\"\"\n",
    "    \n",
    "    # Training performance\n",
    "    train_pred = model.predict(X_train_data)\n",
    "    train_r2 = r2_score(y_train_data, train_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train_data, train_pred))\n",
    "    \n",
    "    # Testing performance\n",
    "    test_pred = model.predict(X_test_data)\n",
    "    test_r2 = r2_score(y_test_data, test_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_data, test_pred))\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Training R¬≤ = {train_r2:.4f} | RMSE = {train_rmse:.4f}\")\n",
    "    print(f\"Testing  R¬≤ = {test_r2:.4f} | RMSE = {test_rmse:.4f}\")\n",
    "    print(f\"Overfitting Check: R¬≤ difference = {train_r2 - test_r2:.4f}\")\n",
    "    \n",
    "    return test_pred\n",
    "\n",
    "y_pred_train = evaluate_model(lm, X_train, X_test, y_train, y_test, \"Linear Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d24ef29",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç 10. Residual Diagnostics ‚Äì Model Validation\n",
    "**Explanation:** Residuals are the differences between actual and predicted values. Analyzing residuals helps us check if our model assumptions are valid. Good residuals should:\n",
    "1. Have no pattern (scattered randomly around zero)\n",
    "2. Follow a normal distribution\n",
    "3. Have constant variance across all predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d79e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Calculate residuals\n",
    "y_test_pred = lm.predict(X_test)\n",
    "residuals = y_test - y_test_pred\n",
    "\n",
    "# Create two diagnostic plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Residuals vs Fitted Values\n",
    "axes[0].scatter(y_test_pred, residuals, alpha=0.6, color='#088395')\n",
    "axes[0].axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel(\"Fitted Values\")\n",
    "axes[0].set_ylabel(\"Residuals\")\n",
    "axes[0].set_title(\"Residuals vs Fitted Values\\n(Check: Random scatter, no pattern)\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Q-Q Plot for normality\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[1])\n",
    "axes[1].set_title(\"Q-Q Plot\\n(Check: Points follow red line = normal residuals)\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5361ed",
   "metadata": {},
   "source": [
    "**Interpretation:** \n",
    "- **Left plot (Residuals vs Fitted):** If points are randomly scattered around the zero line with no pattern, our homoscedasticity assumption is satisfied. Any funnel shape would indicate non-constant variance.\n",
    "- **Right plot (Q-Q Plot):** Points should follow the red diagonal line. Deviations at the tails indicate non-normal residuals, though slight deviations are often acceptable in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf7f25f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ 11. Comparing Multiple Regression Models\n",
    "**Explanation:** Linear regression is a good starting point, but other algorithms may capture non-linear patterns better. We'll compare:\n",
    "- **Ridge Regression**: Adds a penalty for large coefficients to reduce overfitting\n",
    "- **Random Forest**: An ensemble that combines many decision trees\n",
    "- **Gradient Boosting**: Sequentially builds trees to correct previous errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fcb4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Create different models\n",
    "models_dict = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": RidgeCV(alphas=np.logspace(-3, 3, 20), cv=5),\n",
    "    \"Random Forest (100 trees)\": RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, random_state=42, learning_rate=0.1)\n",
    "}\n",
    "\n",
    "# Train and evaluate all models\n",
    "results = {}\n",
    "for name, model in models_dict.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    results[name] = {\"model\": model, \"r2\": r2, \"rmse\": rmse}\n",
    "    print(f\"{name:30s} | R¬≤ = {r2:.4f} | RMSE = {rmse:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e11023",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèÜ 12. Model Comparison Visualization\n",
    "**Explanation:** Comparing models side-by-side helps us choose the best performer. We plot R¬≤ and RMSE values to see which model balances prediction accuracy with simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e88bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "model_names = list(results.keys())\n",
    "r2_scores = [results[name][\"r2\"] for name in model_names]\n",
    "rmse_scores = [results[name][\"rmse\"] for name in model_names]\n",
    "\n",
    "# R¬≤ Comparison\n",
    "axes[0].barh(model_names, r2_scores, color='#05bfdb')\n",
    "axes[0].set_xlabel(\"R¬≤ Score (higher is better)\")\n",
    "axes[0].set_title(\"Model Performance: R¬≤ Comparison\")\n",
    "axes[0].set_xlim([0, 1])\n",
    "for i, v in enumerate(r2_scores):\n",
    "    axes[0].text(v + 0.02, i, f\"{v:.3f}\", va='center')\n",
    "\n",
    "# RMSE Comparison\n",
    "axes[1].barh(model_names, rmse_scores, color='#088395')\n",
    "axes[1].set_xlabel(\"RMSE (lower is better)\")\n",
    "axes[1].set_title(\"Model Performance: RMSE Comparison\")\n",
    "for i, v in enumerate(rmse_scores):\n",
    "    axes[1].text(v + 0.01, i, f\"{v:.3f}\", va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = max(results, key=lambda x: results[x][\"r2\"])\n",
    "print(f\"\\nüèÜ Best performing model: {best_model_name}\")\n",
    "print(f\"   R¬≤ Score: {results[best_model_name]['r2']:.4f}\")\n",
    "print(f\"   RMSE: {results[best_model_name]['rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca17c6dd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìà 13. Feature Importance Analysis\n",
    "**Explanation:** For tree-based models like Random Forest, we can determine which features are most important in predicting house prices. Features used earlier in the tree-splitting process contribute more to predictions. This helps us understand what drives housing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146b94c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance from Random Forest model\n",
    "rf_model = results[\"Random Forest (100 trees)\"][\"model\"]\n",
    "\n",
    "if hasattr(rf_model, 'feature_importances_'):\n",
    "    feature_importance = pd.Series(\n",
    "        rf_model.feature_importances_,\n",
    "        index=X_train.columns\n",
    "    ).sort_values(ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_features = feature_importance.head(8)\n",
    "    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(top_features)))\n",
    "    bars = plt.barh(range(len(top_features)), top_features.values, color=colors)\n",
    "    plt.yticks(range(len(top_features)), top_features.index)\n",
    "    plt.xlabel(\"Feature Importance Score\")\n",
    "    plt.title(\"Top 8 Most Important Features (Random Forest)\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (name, value) in enumerate(top_features.items()):\n",
    "        plt.text(value + 0.005, i, f\"{value:.4f}\", va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nFeature Importance Rankings:\")\n",
    "    print(feature_importance.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b04dc8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß† 14. Key Learnings & Practical Takeaways\n",
    "\n",
    "### Summary of Our Analysis Journey:\n",
    "\n",
    "1. **Exploratory Data Analysis (EDA)** ‚Äì We visualized data distributions, found strong relationships between features, and identified that the California Housing dataset is clean with no missing values.\n",
    "\n",
    "2. **Data Preprocessing** ‚Äì We capped outliers and standardized features to ensure they're on the same scale, which improves model performance.\n",
    "\n",
    "3. **Assumption Checking** ‚Äì We verified that our data reasonably meets linear regression assumptions (linearity, normality, homoscedasticity).\n",
    "\n",
    "4. **Model Training & Evaluation** ‚Äì We trained multiple models and found that ensemble methods (Random Forest, Gradient Boosting) typically outperform simple linear regression on this dataset.\n",
    "\n",
    "5. **Residual Diagnostics** ‚Äì Analyzing residuals confirmed that our model's predictions have reasonable error patterns.\n",
    "\n",
    "6. **Feature Importance** ‚Äì We identified that location-based features (Latitude, Longitude) and median income are key drivers of house prices.\n",
    "\n",
    "### Practical Checklist for Future Projects:\n",
    "‚úì Always explore your data first with visualizations  \n",
    "‚úì Check for and handle missing values and outliers  \n",
    "‚úì Verify regression assumptions before drawing conclusions  \n",
    "‚úì Scale features to comparable ranges  \n",
    "‚úì Split data into train-test sets for honest evaluation  \n",
    "‚úì Compare multiple models instead of assuming one is best  \n",
    "‚úì Analyze residuals to validate model assumptions  \n",
    "‚úì Interpret feature importance to understand your model  \n",
    "\n",
    "This workflow forms a solid foundation that you can apply to any regression problem with tabular data! üéì"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
